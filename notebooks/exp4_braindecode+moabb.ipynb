{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "p8wixPYX53BF",
        "_ToN3lDHOOec"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements"
      ],
      "metadata": {
        "id": "ummHXLxWidL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mne moabb braindecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzjS3_-K8ZuA",
        "outputId": "7b72c8be-dd38-4bdb-acd9-73a39ce078de"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mne in /usr/local/lib/python3.8/dist-packages (1.3.1)\n",
            "Requirement already satisfied: moabb in /usr/local/lib/python3.8/dist-packages (0.4.6)\n",
            "Requirement already satisfied: braindecode in /usr/local/lib/python3.8/dist-packages (0.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from mne) (3.5.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from mne) (2.11.3)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from mne) (1.7.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.8/dist-packages (from mne) (1.22.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from mne) (4.64.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from mne) (23.0)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.8/dist-packages (from mne) (1.6.0)\n",
            "Requirement already satisfied: seaborn>=0.9 in /usr/local/lib/python3.8/dist-packages (from moabb) (0.11.2)\n",
            "Requirement already satisfied: coverage<6.0,>=5.5 in /usr/local/lib/python3.8/dist-packages (from moabb) (5.5)\n",
            "Requirement already satisfied: PyYAML<6.0,>=5.0 in /usr/local/lib/python3.8/dist-packages (from moabb) (5.4.1)\n",
            "Requirement already satisfied: pandas<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from moabb) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from moabb) (1.0.2)\n",
            "Requirement already satisfied: pyriemann>=0.2.6 in /usr/local/lib/python3.8/dist-packages (from moabb) (0.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.15.1 in /usr/local/lib/python3.8/dist-packages (from moabb) (2.25.1)\n",
            "Requirement already satisfied: h5py<4.0,>=3.0 in /usr/local/lib/python3.8/dist-packages (from moabb) (3.1.0)\n",
            "Requirement already satisfied: skorch in /usr/local/lib/python3.8/dist-packages (from braindecode) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (4.38.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas<2.0,>=1.0->moabb) (2022.7.1)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.5->mne) (1.4.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from pyriemann>=0.2.6->moabb) (1.2.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.15.1->moabb) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.15.1->moabb) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.15.1->moabb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.15.1->moabb) (2022.12.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<2.0,>=1.0->moabb) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->mne) (2.0.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.8/dist-packages (from skorch->braindecode) (0.8.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib->mne) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import mne\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import copy\n",
        "\n",
        "from braindecode import EEGClassifier\n",
        "from braindecode.datasets import create_from_X_y\n",
        "from braindecode.models import ShallowFBCSPNet, EEGNetv4\n",
        "from braindecode.util import set_random_seeds\n",
        "from skorch.callbacks import LRScheduler\n",
        "from braindecode.preprocessing import preprocess, Preprocessor\n",
        "\n",
        "from moabb.datasets import BNCI2014001, PhysionetMI\n",
        "from moabb.evaluations import WithinSessionEvaluation, CrossSubjectEvaluation\n",
        "from moabb.paradigms import LeftRightImagery\n",
        "from moabb.utils import set_download_dir\n",
        "\n",
        "import numpy as np\n",
        "from numpy import unique, iscomplexobj, real, any, isfinite\n",
        "from scipy.linalg import sqrtm, inv\n",
        "\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n"
      ],
      "metadata": {
        "id": "2f131wWH8hc_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_download_dir(osp.join(osp.expanduser(\"~\"), \"mne_data\"))"
      ],
      "metadata": {
        "id": "uryZ7TTqu1xp"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Euclidean Alignment as a transformation"
      ],
      "metadata": {
        "id": "p8wixPYX53BF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EuclideanAlignment:\n",
        "    \"\"\"\n",
        "    https://github.com/mcd4874/NeurIPS_competition/blob/4df1f222929e9824a55c9c4ae6634743391b0fe9/EEG_Lightning/dassl/data/datasets/data_util.py#L218\n",
        "    convert trials of each subject to a new format with Euclidean Alignment technique\n",
        "    https://arxiv.org/pdf/1808.05464.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    # Define Attributes\n",
        "    def __init__(self, metadata, list_r_op=None,subject_ids=None):\n",
        "      # list_r_op : list of each individual's reference matrix\n",
        "        self.groups = metadata.subject.values\n",
        "        self.runs = metadata.run.values\n",
        "        self.list_r_op = list_r_op\n",
        "        if subject_ids is not None:\n",
        "            update_list_r_op = [self.list_r_op[subject_id] for subject_id in subject_ids]\n",
        "            print(\"only use r-op for subjects {}\".format(subject_ids))\n",
        "            self.list_r_op = update_list_r_op\n",
        "\n",
        "    # Calculate ref matrix\n",
        "    def calculate_r_op(self,data):\n",
        "\n",
        "        assert len(data.shape) == 3\n",
        "\n",
        "        # compute mean covariance\n",
        "        r = 0\n",
        "        for trial in data:\n",
        "            cov = np.cov(trial, rowvar=True)\n",
        "            r += cov\n",
        "\n",
        "        r = r/data.shape[0]\n",
        "\n",
        "        if np.iscomplexobj(r):\n",
        "            print(\"covariance matrix problem\")\n",
        "        if np.iscomplexobj(sqrtm(r)):\n",
        "            print(\"covariance matrix problem sqrt\")\n",
        "\n",
        "        # compute ^-1/2\n",
        "        r_op = inv(sqrtm(r))\n",
        "\n",
        "        if np.iscomplexobj(r_op):\n",
        "            print(\"WARNING! Covariance matrix was not SPD somehow. Can be caused by running ICA-EOG rejection, if \"\n",
        "                  \"not, check data!!\")\n",
        "            r_op = np.real(r_op).astype(np.float64)\n",
        "        elif not np.any(np.isfinite(r_op)):\n",
        "            print(\"WARNING! Not finite values in R Matrix\")\n",
        "        return r_op\n",
        "\n",
        "    # Matrix multiplication\n",
        "    def convert_trials(self,data,r_op):\n",
        "        results = np.matmul(r_op, data)\n",
        "        return results\n",
        "\n",
        "    # reference matrix for each subject\n",
        "    def generate_list_r_op(self,subjects_data):\n",
        "        list_r_op = list()\n",
        "        for subject_idx in range(len(subjects_data)):\n",
        "            subject_data = subjects_data[subject_idx]\n",
        "            r_op = self.calculate_r_op(subject_data)\n",
        "            list_r_op.append(r_op)\n",
        "        return list_r_op\n",
        "\n",
        "    # Apply the EA in all data\n",
        "    def convert_all_data_with_EA(self,subjects_data):\n",
        "        #calculate r_op for each subject (or run)\n",
        "        \n",
        "        if self.list_r_op is not None:\n",
        "            assert len(self.list_r_op) == len(subjects_data)\n",
        "            print(\"use exist r_op\")\n",
        "\n",
        "        else:\n",
        "            print(\"generate new r_op\")\n",
        "            self.list_r_op = self.generate_list_r_op(subjects_data)\n",
        "\n",
        "        new_data = list()\n",
        "\n",
        "        for subject_idx in range(len(subjects_data)):\n",
        "\n",
        "            subject_data = subjects_data[subject_idx]\n",
        "            r_op = self.list_r_op[subject_idx]\n",
        "            subject_data = self.convert_trials(subject_data,r_op)\n",
        "            new_data.append(subject_data)\n",
        "\n",
        "        return new_data\n",
        "\n",
        "    # Apply the EA on subject_data\n",
        "    def convert_subject_data_with_EA(self, subject_data):\n",
        "        # calculate r_op for a subject (or run)\n",
        "        r_op = self.calculate_r_op(subject_data)\n",
        "        subject_data = self.convert_trials(subject_data,r_op)\n",
        "        return subject_data, r_op\n",
        "\n",
        "    # Apply the EA on a dataset on each run from all subjects\n",
        "    def apply_EA(self, X, subjects_idx, n_runs):\n",
        "      \n",
        "      concat=[]; r_op_list=[]\n",
        "\n",
        "      for subj in subjects_idx:\n",
        "        print(f\"Subject {subj}\")\n",
        "        X_subj, r_op = self.apply_EA_subj( X, subj, n_runs = n_runs )\n",
        "        concat.append(X_subj)\n",
        "        r_op_list.append(r_op)\n",
        "\n",
        "      X_EA=np.concatenate(concat)\n",
        "\n",
        "      return X_EA, r_op_list\n",
        "\n",
        "    # Apply the EA on a subject\n",
        "    def apply_EA_subj(self, X, subj, separate_runs = True, n_runs=None, run=None):\n",
        "\n",
        "      data_subj = self.groups==subj\n",
        "\n",
        "      #If we want to align each run\n",
        "      if separate_runs == True:\n",
        "        if run is None:\n",
        "          concat=[]; r_op_list = []\n",
        "\n",
        "          for k in range(n_runs):\n",
        "            A = self.runs\n",
        "            data_runs = A==f'run_{k}'\n",
        "            intersec = np.logical_and(data_subj, data_runs)\n",
        "            X_aux = copy.deepcopy(X[intersec])\n",
        "            X_aux_EA, r_op = self.convert_subject_data_with_EA(X_aux)\n",
        "            concat.append(X_aux_EA)\n",
        "\n",
        "          X_EA = np.concatenate(concat)\n",
        "          r_op_list.append(r_op)\n",
        "\n",
        "        else:\n",
        "          data_runs = self.runs==f'run_{run}'\n",
        "\n",
        "          intersec = np.logical_and(data_subj, data_runs)\n",
        "          X_aux = copy.deepcopy(X[intersec])\n",
        "          X_EA, r_op_list = self.convert_subject_data_with_EA(X_aux)\n",
        "      \n",
        "      else:\n",
        "        X_subj = X[data_subj]\n",
        "        X_EA, r_op = self.convert_subject_data_with_EA(X_subj)\n",
        "\n",
        "      return X_EA, r_op_list\n",
        "\n"
      ],
      "metadata": {
        "id": "DoHVYJsm8UeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "from scipy.linalg import sqrtm, inv\n",
        "from numpy import iscomplexobj, real, any, isfinite\n",
        "### Define this as Transformation, maybe...\n",
        "def euclidean_alignment(data, y=None):\n",
        "\n",
        "  data_EA=copy.deepcopy(data)\n",
        "\n",
        "  assert len(data.shape) == 3\n",
        "\n",
        "  print(len(data))\n",
        "\n",
        "  r = 0\n",
        "  for trial in data:\n",
        "    cov = np.cov(trial, rowvar=True)\n",
        "    r += cov\n",
        "\n",
        "  r = r/len(data)\n",
        "\n",
        "  if iscomplexobj(r):\n",
        "    print(\"covariance matrix problem\")\n",
        "  if iscomplexobj(sqrtm(r)):\n",
        "    print(\"covariance matrix problem sqrt\")\n",
        "\n",
        "  r_op = inv(sqrtm(r))\n",
        "\n",
        "  if iscomplexobj(r_op):\n",
        "    print(\"WARNING! Covariance matrix was not SPD somehow. \"+\n",
        "            \"Can be caused by running ICA-EOG rejection, if \"+\n",
        "            \"not, check data!!\")\n",
        "    r_op = real(r_op).astype(np.float64)\n",
        "  elif not any(isfinite(r_op)):\n",
        "    print(\"WARNING! Not finite values in R Matrix\")\n",
        "\n",
        "  result = np.matmul(r_op, data)\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "# dataset_EA = preprocess(dataset,[Preprocessor(euclidean_alignment,apply_on_array=True)])\n"
      ],
      "metadata": {
        "id": "2jRRBFlUZwqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_runs_EA(X,rpc, n_classes):\n",
        "  X_aux = []\n",
        "  m = rpc*n_classes\n",
        "  n = X.shape[0]\n",
        "  for k in range(int(n/m)):\n",
        "    run = X[k*m:(k+1)*m]\n",
        "    run_EA = euclidean_alignment(run)\n",
        "    X_aux.append(run_EA)\n",
        "  X_EA = np.concatenate(X_aux)\n",
        "  return X_EA\n",
        "  "
      ],
      "metadata": {
        "id": "h9XU1Ipwfdls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Cross-Subject Evaluation divides the test individual's sessions? How to use both?"
      ],
      "metadata": {
        "id": "Sf49d27JxtlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "A = runs==f'run_{i}'\n",
        "B = groups==1\n",
        "C = np.logical_and(A,B)\n",
        "B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLLaWKRBV5I_",
        "outputId": "62477e3c-9891-484a-e478-22bd160d0e23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True,  True,  True, ..., False, False, False])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "groups = metadata.subject.values\n",
        "sessions = metadata.session.values\n",
        "runs = metadata.run.values\n",
        "subjects_idx =dataset_a.subject_list\n",
        "n_runs = 6\n",
        "EA = EuclideanAlignment(metadata)\n",
        "X_EA,_ = EA.apply_EA(X, subjects_idx, n_runs)"
      ],
      "metadata": {
        "id": "miHTvAKuOuaE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a04c86c0-f6d7-4e5e-a243-fe1ddeb42765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject 1\n",
            "Subject 2\n",
            "Subject 3\n",
            "Subject 4\n",
            "Subject 5\n",
            "Subject 6\n",
            "Subject 7\n",
            "Subject 8\n",
            "Subject 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_EA"
      ],
      "metadata": {
        "id": "tM04selnaywq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "EULoNIK6a0gD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load datasets"
      ],
      "metadata": {
        "id": "SzP1-hJ16BEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Evaluation\n",
        "paradigm = LeftRightImagery()\n",
        "# Because this is being auto-generated we only use 2 subjects\n",
        "dataset_a = BNCI2014001()\n",
        "dataset_b = PhysionetMI(imagined=True)\n",
        "\n",
        "#dataset.subject_list = dataset.subject_list[:9]\n",
        "# Por que criamos essa lista datasets? \n",
        "# Fazendo isso os modelos vao ser criados entre os proprios datasets ou vai misturar?\n",
        "datasets = [dataset_a]"
      ],
      "metadata": {
        "id": "I2yaZX0p7dU4"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To obtain the number of channels and input windows samples, can we hard-code with the right paramethers (that are known) or do we need to extract them?"
      ],
      "metadata": {
        "id": "ryokUKFVKBwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y, metadata = paradigm.get_data(dataset_a)"
      ],
      "metadata": {
        "id": "35GUDbxZJ_-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b8qwIoL6oEj",
        "outputId": "f3ec5825-25d8-46ea-8da0-2e3a9ff1a82a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2592, 22, 1001)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "groups = metadata.subject.values\n",
        "sessions = metadata.session.values\n",
        "n_subjects = len(dataset_a.subject_list)\n"
      ],
      "metadata": {
        "id": "_v-d4iM9jyGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groups.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgpkDFbO7Bvx",
        "outputId": "676dee54-8a66-4709-b551-3dd065fddbfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2592,)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(groups)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0XlJHCBk7-u",
        "outputId": "ac0bef8c-b7b5-4f29-db06-0a1ec362452b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 3, 4, 5, 6, 7, 8, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from tqdm import tqdm\n",
        "\n",
        "loo = LeaveOneGroupOut()\n",
        "for test, train in tqdm(\n",
        "                loo.split(X, y, groups),\n",
        "                total=n_subjects,\n",
        "                desc=f\"{dataset_a.code}-WithinSubject\"):\n",
        "\n",
        "  print(test.shape)\n",
        "  ix = groups[test] == 1\n",
        "  print(X[test[ix]].shape)\n",
        "  print(X[train].shape)"
      ],
      "metadata": {
        "id": "_WGAnDQcdBPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[:24].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e56KDx5eaQN",
        "outputId": "25e3fca1-c237-4c65-fecd-6255f5819719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24, 22, 1001)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_EA = split_runs_EA(X, 12, 2)"
      ],
      "metadata": {
        "id": "EDZZhgZYhAfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrVW1naPeXhw",
        "outputId": "d56ca7bc-cfab-499b-88d8-9a21afa55450"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2592, 22, 1001)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "2592/9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "np1iDeyzeceN",
        "outputId": "6c71098e-63ac-4fc3-f29c-c99b7270d960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "288.0"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(2592/9)/2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vK7ck3cehFZ",
        "outputId": "f9421082-8b50-4bf0-abf7-8f28567f76ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "144.0"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWVAhk6kPg3A",
        "outputId": "14b8083c-5cb7-498b-ce02-a4d4ef522657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWpcQcXZIFhy",
        "outputId": "5a946f53-1541-4806-d566-22b3be286cae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create model and classifier"
      ],
      "metadata": {
        "id": "B-LygOVP6EUz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some doubts: thw 2 datasets we'rw using have different number of channels and classes. How do we define the number of channels we're going to use?? How to select the channels?\n",
        "\n",
        "* Tem como definir uma sequência? Tipo, primeiro cria e avalia p/ BBNCI201400 e depois p Physionet?"
      ],
      "metadata": {
        "id": "VegzNMo0cvq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_det_device():\n",
        "  cuda = (\n",
        "      torch.cuda.is_available()\n",
        "  )  # check if GPU is available, if True chooses to use it\n",
        "  device = \"cuda\" if cuda else \"cpu\"\n",
        "  if cuda:\n",
        "      torch.backends.cudnn.benchmark = True\n",
        "  seed = 20200220  # random seed to make results reproducible\n",
        "  # Set random seed to be able to reproduce results\n",
        "  set_random_seeds(seed=seed, cuda=cuda)\n",
        "  return device"
      ],
      "metadata": {
        "id": "0nWyMpAl7dIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda = (\n",
        "    torch.cuda.is_available()\n",
        ")  # check if GPU is available, if True chooses to use it\n",
        "device = \"cuda\" if cuda else \"cpu\"\n",
        "if cuda:\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "seed = 20200220  # random seed to make results reproducible\n",
        "# Set random seed to be able to reproduce results\n",
        "set_random_seeds(seed=seed, cuda=cuda)\n",
        "\n",
        "n_classes = 2\n",
        "\n",
        "# hard-coded for now\n",
        "# How to adjust??\n",
        "# PARAMETHERS FOR THE BBNCI2014001 DATASET\n",
        "n_chans = 22\n",
        "input_window_samples = 1001\n",
        "\n",
        "'''\n",
        "# PARAMETHERS FOR THE PHYSIONET DATASET\n",
        "n_chans = 64\n",
        "input_window_samples = 481\n",
        "'''\n",
        "\n",
        "model = EEGNetv4(\n",
        "    n_chans,\n",
        "    n_classes,\n",
        "    input_window_samples=input_window_samples,\n",
        "    final_conv_length='auto',\n",
        "    drop_prob=0.5\n",
        ")\n",
        "\n",
        "\n",
        "# Send model to GPU\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "\n",
        "# These values we found good for shallow network:\n",
        "lr = 0.0625 * 0.01\n",
        "weight_decay = 0\n",
        "\n",
        "batch_size = 64\n",
        "n_epochs = 100\n",
        "\n",
        "clf = EEGClassifier(\n",
        "    model,\n",
        "    criterion=torch.nn.NLLLoss,\n",
        "    optimizer=torch.optim.AdamW,\n",
        "    train_split=None,  # using valid_set for validation\n",
        "    optimizer__lr=lr,\n",
        "    optimizer__weight_decay=weight_decay,\n",
        "    batch_size=batch_size,\n",
        "    callbacks=[\n",
        "        \"accuracy\",\n",
        "        (\"lr_scheduler\", LRScheduler(\"CosineAnnealingLR\", T_max=n_epochs - 1)),\n",
        "    ],\n",
        "    device=device,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JRBzIeO-rim",
        "outputId": "b167fe8a-8be2-413d-a031-15f156e121ca"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/braindecode/util.py:51: UserWarning: torch.backends.cudnn.benchmark was set to True which may results in lack of reproducibility. In some cases to ensure reproducibility you may need to set torch.backends.cudnn.benchmark to False.\n",
            "  warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Idea: (since some parameters are different for different datasets)"
      ],
      "metadata": {
        "id": "s07IbDVO6c0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_model( n_chans, n_classes, input_window_samples):\n",
        "  model = EEGNetv4(\n",
        "    n_chans,\n",
        "    n_classes,\n",
        "    input_window_samples=input_window_samples,\n",
        "    final_conv_length='auto',\n",
        "    drop_prob=0.5\n",
        "  )\n",
        "  return model"
      ],
      "metadata": {
        "id": "mob__sqy6ccV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define pipeline\n",
        "\n",
        "Why is it important? Define sequence of transformations\n",
        "\n",
        "Can we use the make_pipeline func?"
      ],
      "metadata": {
        "id": "ePhg7FDakNZY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "G1uRIVHI7-fU"
      },
      "outputs": [],
      "source": [
        "class TransformaParaWindowsDataset(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, kw_args=None):\n",
        "        self.kw_args = kw_args\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.y = y\n",
        "        # self.X = euclidean_alignment(X.get_data())\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None ):\n",
        "        '''\n",
        "        metadata = X.metadata\n",
        "        X_aux = X.get_data()    \n",
        "\n",
        "        subjects_idx = list(range(1,10))\n",
        "        n_runs = 6\n",
        "        EA = EuclideanAlignment(metadata)\n",
        "        X_EA,_ = EA.apply_EA( X_aux, subjects_idx, n_runs)\n",
        "        '''\n",
        "\n",
        "        #X_EA = split_runs_EA(X.get_data(), 12, 2)\n",
        "\n",
        "        dataset = create_from_X_y(\n",
        "            #X=X_EA,\n",
        "            X=X.get_data(),\n",
        "            y=self.y,\n",
        "            window_size_samples=X.get_data().shape[2],\n",
        "            window_stride_samples=X.get_data().shape[2],\n",
        "            drop_last_window=False,\n",
        "            sfreq=X.info[\"sfreq\"],\n",
        "        )\n",
        "\n",
        "        # dataset_EA = preprocess(dataset,[Preprocessor(euclidean_alignment,apply_on_array=True)])\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def __sklearn_is_fitted__(self):\n",
        "        \"\"\"Return True since Transfomer is stateless.\"\"\"\n",
        "        return True\n",
        "\n",
        "\n",
        "class TransformaParaWindowsDatasetEA(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, kw_args=None):\n",
        "        self.kw_args = kw_args\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.y = y\n",
        "        # self.X = euclidean_alignment(X.get_data())\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None ):\n",
        "\n",
        "        X_EA = split_runs_EA(X.get_data(), 12, 2)\n",
        "\n",
        "        dataset = create_from_X_y(\n",
        "            X=X_EA,\n",
        "            y=self.y,\n",
        "            window_size_samples=X.get_data().shape[2],\n",
        "            window_stride_samples=X.get_data().shape[2],\n",
        "            drop_last_window=False,\n",
        "            sfreq=X.info[\"sfreq\"],\n",
        "        )\n",
        "\n",
        "        # dataset_EA = preprocess(dataset,[Preprocessor(euclidean_alignment,apply_on_array=True)])\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def __sklearn_is_fitted__(self):\n",
        "        \"\"\"Return True since Transfomer is stateless.\"\"\"\n",
        "        return True\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassifierModel(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, clf, kw_args=None):\n",
        "        self.clf = clf\n",
        "        self.classes_ = None\n",
        "        self.kw_args = kw_args\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.clf.fit(X, y=y, **self.kw_args)\n",
        "        self.classes_ = unique(y)\n",
        "\n",
        "        return self.clf\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.clf.predict(X)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return self.clf.predict_proba(X)\n",
        "\n"
      ],
      "metadata": {
        "id": "io894ZuE_kxy"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if euclidean alignment:\n",
        "# create_dataset = TransformaParaWindowsDatasetEA()\n",
        "create_dataset = TransformaParaWindowsDataset()\n",
        "\n",
        "fit_params = {\"epochs\": 100}\n",
        "\n",
        "brain_clf = ClassifierModel(clf, fit_params)\n",
        "\n",
        "pipe = Pipeline([(\"Braindecode_dataset\", create_dataset), \n",
        "                 (\"Net\", brain_clf)])\n",
        "print(pipe)\n",
        "pipes = {}\n",
        "pipes[\"EEGNetv4\"] = pipe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkfxH_5YbLyI",
        "outputId": "a949c510-6b2d-45de-85ae-d937632dde65"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline(steps=[('Braindecode_dataset', TransformaParaWindowsDataset()),\n",
            "                ('Net',\n",
            "                 ClassifierModel(clf=<class 'braindecode.classifier.EEGClassifier'>[uninitialized](\n",
            "  module=EEGNetv4(\n",
            "    (ensuredims): Ensure4d()\n",
            "    (dimshuffle): Expression(expression=_transpose_to_b_1_c_0) \n",
            "    (conv_temporal): Conv2d(1, 8, kernel_size=(1, 64), stride=(1, 1), padding=(0, 32), bias=False)\n",
            "    (bnorm_tem...\n",
            "    (elu_2): Expression(expression=elu) \n",
            "    (pool_2): AvgPool2d(kernel_size=(1, 8), stride=(1, 8), padding=0)\n",
            "    (drop_2): Dropout(p=0.5, inplace=False)\n",
            "    (conv_classifier): Conv2d(16, 2, kernel_size=(1, 31), stride=(1, 1))\n",
            "    (softmax): LogSoftmax(dim=1)\n",
            "    (permute_back): Expression(expression=_transpose_1_0) \n",
            "    (squeeze): Expression(expression=squeeze_final_output) \n",
            "  ),\n",
            "),\n",
            "                                 kw_args={'epochs': 100}))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Some tests with paradigm methods**\n",
        "\n",
        "How can I use this to separate X_trains?"
      ],
      "metadata": {
        "id": "_ToN3lDHOOec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a=np.array([1,2])\n",
        "b=np.array([3,4])\n",
        "c=np.concatenate([a,b])\n",
        "c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjoRj-TQpJ9v",
        "outputId": "49feca5c-a1b5-47da-f097-4477469ce89d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 3, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paradigm = LeftRightImagery()\n",
        "# Because this is being auto-generated we only use 2 subjects\n",
        "dataset = BNCI2014001()\n",
        "dataset_b = PhysionetMI(imagined=True)"
      ],
      "metadata": {
        "id": "SNIMybV5LrAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOauLbMVJqBd",
        "outputId": "59ae5e13-2080-4fe3-a2ba-caf4e172b4bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2592, 22, 1001)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y, metadata = paradigm.get_data(dataset_a)"
      ],
      "metadata": {
        "id": "Oc_3mPMvFz4y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f216b41e-3df1-43d0-8d2f-815b39c01153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/moabb/datasets/download.py:53: RuntimeWarning: Setting non-standard config type: \"MNE_DATASETS_BNCI_PATH\"\n",
            "  set_config(key, get_config(\"MNE_DATA\"))\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A01T.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A01T.mat'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNE_DATA is not already configured. It will be set to default location in the home directory - /root/mne_data\n",
            "All datasets will be downloaded to this location, if anything is already downloaded, please move manually to this location\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 42.8M/42.8M [00:00<00:00, 29.8GB/s]\n",
            "SHA256 hash of downloaded file: 054f02e70cf9c4ada1517e9b9864f45407939c1062c6793516585c6f511d0325\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A01E.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A01E.mat'.\n",
            "100%|█████████████████████████████████████| 43.8M/43.8M [00:00<00:00, 24.1GB/s]\n",
            "SHA256 hash of downloaded file: 53d415f39c3d7b0c88b894d7b08d99bcdfe855ede63831d3691af1a45607fb62\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A02T.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A02T.mat'.\n",
            "100%|█████████████████████████████████████| 43.1M/43.1M [00:00<00:00, 30.2GB/s]\n",
            "SHA256 hash of downloaded file: 5ddd5cb520b1692c3ba1363f48d98f58f0e46f3699ee50d749947950fc39db27\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A02E.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A02E.mat'.\n",
            "100%|█████████████████████████████████████| 44.2M/44.2M [00:00<00:00, 24.8GB/s]\n",
            "SHA256 hash of downloaded file: d63c454005d3a9b41d8440629482e855afc823339bdd0b5721842a7ee9cc7b12\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A03T.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A03T.mat'.\n",
            "100%|█████████████████████████████████████| 44.1M/44.1M [00:00<00:00, 32.1GB/s]\n",
            "SHA256 hash of downloaded file: 7e731ee8b681d5da6ecb11ae1d4e64b1653c7f15aad5d6b7620b25ce53141e80\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A03E.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A03E.mat'.\n",
            "100%|█████████████████████████████████████| 42.3M/42.3M [00:00<00:00, 25.6GB/s]\n",
            "SHA256 hash of downloaded file: d4229267ec7624fa8bd3af5cbebac17f415f7c722de6cb676748f8cb3b717d97\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A04T.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A04T.mat'.\n",
            "100%|█████████████████████████████████████| 37.2M/37.2M [00:00<00:00, 11.1GB/s]\n",
            "SHA256 hash of downloaded file: 15850d81b95fc88cc8b9589eb9b713d49fa071e28adaf32d675b3eaa30591d6e\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A04E.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A04E.mat'.\n",
            "100%|█████████████████████████████████████| 41.7M/41.7M [00:00<00:00, 25.8GB/s]\n",
            "SHA256 hash of downloaded file: 81916dff2c12997974ba50ffc311da006ea66e525010d010765f0047e771c86a\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A05T.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A05T.mat'.\n",
            "100%|█████████████████████████████████████| 42.5M/42.5M [00:00<00:00, 23.8GB/s]\n",
            "SHA256 hash of downloaded file: 77387d3b669f4ed9a7c1dac4dcba4c2c40c8910bae20fb961bb7cf5a94912950\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A05E.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A05E.mat'.\n",
            "100%|█████████████████████████████████████| 44.4M/44.4M [00:00<00:00, 32.4GB/s]\n",
            "SHA256 hash of downloaded file: 8b357470865610c28b2f1d351beac247a56a856f02b2859d650736eb2ef77808\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A06T.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A06T.mat'.\n",
            "100%|█████████████████████████████████████| 44.6M/44.6M [00:00<00:00, 26.6GB/s]\n",
            "SHA256 hash of downloaded file: 4dc3be1b0d60279134d1220323c73c68cf73799339a7fb224087a3c560a9a7e2\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A06E.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A06E.mat'.\n",
            "100%|█████████████████████████████████████| 43.4M/43.4M [00:00<00:00, 20.8GB/s]\n",
            "SHA256 hash of downloaded file: bf67a40621b74b6af7a986c2f6edfff7fc2bbbca237aadd07b575893032998d1\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A07T.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A07T.mat'.\n",
            "100%|█████████████████████████████████████| 42.8M/42.8M [00:00<00:00, 22.5GB/s]\n",
            "SHA256 hash of downloaded file: 43b6bbef0be78f0ac2b66cb2d9679091f1f5b7f0a5d4ebef73d2c7cc8e11aa96\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A07E.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A07E.mat'.\n",
            "100%|█████████████████████████████████████| 42.2M/42.2M [00:00<00:00, 9.15GB/s]\n",
            "SHA256 hash of downloaded file: b9aaec73dcee002fab84ee98e938039a67bf6a3cbf4fc86d5d8df198cfe4c323\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A08T.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A08T.mat'.\n",
            "100%|█████████████████████████████████████| 45.0M/45.0M [00:00<00:00, 27.4GB/s]\n",
            "SHA256 hash of downloaded file: 7a4b3bd602d5bc307d3f4527fca2cf076659e94aca584dd64f6286fd413a82f2\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A08E.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A08E.mat'.\n",
            "100%|█████████████████████████████████████| 46.3M/46.3M [00:00<00:00, 23.9GB/s]\n",
            "SHA256 hash of downloaded file: 0eedbd89790c7d621c8eef68065ddecf80d437bbbcf60321d9253e2305f294f7\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A09T.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A09T.mat'.\n",
            "100%|█████████████████████████████████████| 44.8M/44.8M [00:00<00:00, 14.7GB/s]\n",
            "SHA256 hash of downloaded file: b28d8a262c779c8cad9cc80ee6aa9c5691cfa6617c03befe490a090347ebd15c\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A09E.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A09E.mat'.\n",
            "100%|█████████████████████████████████████| 44.8M/44.8M [00:00<00:00, 25.2GB/s]\n",
            "SHA256 hash of downloaded file: 5d79649a42df9d51215def8ffbdaf1c3f76c54b88b9bbaae721e8c6fd972cc36\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_b.subject_list=subj = list(range(1,4))"
      ],
      "metadata": {
        "id": "QNrN54OFLwx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xb, yb, metadatab = paradigm.get_data(dataset_b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlyhfOiiY_9h",
        "outputId": "a6bcb8a0-3e24-445e-fc06-d9aae2d01838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading data from 'https://physionet.org/files/eegmmidb/1.0.0/S003/S003R04.edf' to file '/root/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S003/S003R04.edf'.\n",
            "100%|█████████████████████████████████████| 2.60M/2.60M [00:00<00:00, 1.85GB/s]\n",
            "SHA256 hash of downloaded file: 7d0732eea963488a53153835524e55c2b68220b0a0c7c5be99e535a9f5367e7f\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'https://physionet.org/files/eegmmidb/1.0.0/S003/S003R08.edf' to file '/root/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S003/S003R08.edf'.\n",
            "100%|█████████████████████████████████████| 2.60M/2.60M [00:00<00:00, 2.05GB/s]\n",
            "SHA256 hash of downloaded file: 0f50978bb972e693b8c758a9223a2d9fa35c7f117226391090bcc32a83ce765d\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'https://physionet.org/files/eegmmidb/1.0.0/S003/S003R12.edf' to file '/root/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S003/S003R12.edf'.\n",
            "100%|██████████████████████████████████████| 2.60M/2.60M [00:00<00:00, 995MB/s]\n",
            "SHA256 hash of downloaded file: bbf7137bfa7905724741e95359fa090439d4422c07bdba16c792acb09ebd6421\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'https://physionet.org/files/eegmmidb/1.0.0/S003/S003R06.edf' to file '/root/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S003/S003R06.edf'.\n",
            "100%|█████████████████████████████████████| 2.60M/2.60M [00:00<00:00, 1.58GB/s]\n",
            "SHA256 hash of downloaded file: 8d48a46397416bbea19eb6b97474aaade72029364231202295bbda805ed79c97\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'https://physionet.org/files/eegmmidb/1.0.0/S003/S003R10.edf' to file '/root/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S003/S003R10.edf'.\n",
            "100%|█████████████████████████████████████| 2.60M/2.60M [00:00<00:00, 1.51GB/s]\n",
            "SHA256 hash of downloaded file: 99429f0075f59216a10b75eea4029a7ded7bccec34d17100b08a55770cf1f014\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'https://physionet.org/files/eegmmidb/1.0.0/S003/S003R14.edf' to file '/root/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S003/S003R14.edf'.\n",
            "100%|█████████████████████████████████████| 2.60M/2.60M [00:00<00:00, 2.01GB/s]\n",
            "SHA256 hash of downloaded file: c80a7a0fab93074cdead76450b49ba8d27b7183f1baa406daf5207d2c1825194\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "WARNING:moabb.paradigms.base:No matching annotations in ('/root/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S001/S001R06.edf',)\n",
            "WARNING:moabb.paradigms.base:No matching annotations in ('/root/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S001/S001R10.edf',)\n",
            "WARNING:moabb.paradigms.base:No matching annotations in ('/root/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S001/S001R14.edf',)\n",
            "WARNING:moabb.paradigms.base:No matching annotations in ('/root/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S002/S002R06.edf',)\n",
            "WARNING:moabb.paradigms.base:No matching annotations in ('/root/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S002/S002R10.edf',)\n",
            "WARNING:moabb.paradigms.base:No matching annotations in ('/root/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S002/S002R14.edf',)\n",
            "WARNING:moabb.paradigms.base:No matching annotations in ('/root/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S003/S003R06.edf',)\n",
            "WARNING:moabb.paradigms.base:No matching annotations in ('/root/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S003/S003R10.edf',)\n",
            "WARNING:moabb.paradigms.base:No matching annotations in ('/root/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S003/S003R14.edf',)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPqeOoKEZSjg",
        "outputId": "b60f9174-9a2c-4717-a219-38a3e89f9a42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(135, 64, 481)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(90, 64, 481)"
      ],
      "metadata": {
        "id": "hAITtSorbE0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t016Nob2W-B8",
        "outputId": "89a81662-bbe5-45f3-8f34-b7b95b925790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<moabb.datasets.bnci.BNCI2014001 at 0x7f235d0cea90>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "X.shape = (a,b,c)\n",
        "\n",
        "a : epochs (number of trials)\n",
        "\n",
        "b : channels \n",
        "\n",
        "c : time sample \n",
        "\n",
        "If the number of channels and time samples of each dataset are different, how can I define the model? "
      ],
      "metadata": {
        "id": "jEFsEqSJOVc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "mHHhgkwzMnT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_b.shape"
      ],
      "metadata": {
        "id": "6YGwO7BOMpIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "id": "gw281wVbSuWN",
        "outputId": "6d07fa6e-18bb-4044-f9c3-498cc3249843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      subject    session    run\n",
              "0           1  session_T  run_0\n",
              "1           1  session_T  run_0\n",
              "2           1  session_T  run_0\n",
              "3           1  session_T  run_0\n",
              "4           1  session_T  run_0\n",
              "...       ...        ...    ...\n",
              "2587        9  session_E  run_5\n",
              "2588        9  session_E  run_5\n",
              "2589        9  session_E  run_5\n",
              "2590        9  session_E  run_5\n",
              "2591        9  session_E  run_5\n",
              "\n",
              "[2592 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8ae7828c-4343-42be-9c7e-f3a0f343f1d5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subject</th>\n",
              "      <th>session</th>\n",
              "      <th>run</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>session_T</td>\n",
              "      <td>run_0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>session_T</td>\n",
              "      <td>run_0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>session_T</td>\n",
              "      <td>run_0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>session_T</td>\n",
              "      <td>run_0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>session_T</td>\n",
              "      <td>run_0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2587</th>\n",
              "      <td>9</td>\n",
              "      <td>session_E</td>\n",
              "      <td>run_5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2588</th>\n",
              "      <td>9</td>\n",
              "      <td>session_E</td>\n",
              "      <td>run_5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2589</th>\n",
              "      <td>9</td>\n",
              "      <td>session_E</td>\n",
              "      <td>run_5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2590</th>\n",
              "      <td>9</td>\n",
              "      <td>session_E</td>\n",
              "      <td>run_5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2591</th>\n",
              "      <td>9</td>\n",
              "      <td>session_E</td>\n",
              "      <td>run_5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2592 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8ae7828c-4343-42be-9c7e-f3a0f343f1d5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8ae7828c-4343-42be-9c7e-f3a0f343f1d5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8ae7828c-4343-42be-9c7e-f3a0f343f1d5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "groups = metadata.subject.values\n",
        "sessions = metadata.session.values\n",
        "runs = metadata.run.values\n",
        "n_subjects = len(dataset_a.subject_list)"
      ],
      "metadata": {
        "id": "uxzUKk-sTOyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "def X_subj(X,subj,run=None):\n",
        "  data_subj = groups==subj\n",
        "\n",
        "  if run is None:\n",
        "    for k in range(6):\n",
        "      data_runs = runs==f'run_{k}'\n",
        "\n",
        "      intersec = np.logical_and(data_subj, data_runs)\n",
        "      X_aux = copy.deepcopy(X[intersec])\n",
        "\n",
        "  else:\n",
        "    data_runs = runs==f'run_{run}'\n",
        "\n",
        "    intersec = np.logical_and(data_subj, data_runs)\n",
        "    X_aux = copy.deepcopy(X[intersec])\n",
        "\n",
        "  return X_aux\n",
        "X_subj_0 = X_subj(X,1,0)\n",
        "X_subj_1 = X_subj(X,1,1)"
      ],
      "metadata": {
        "id": "awkx3kM3YyiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_a = np.concatenate([X_subj_0, X_subj_1])"
      ],
      "metadata": {
        "id": "3WswZvUEcgwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_a.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRN0nYL_croD",
        "outputId": "29239648-e751-45ed-c30e-ee65e1603990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(96, 22, 1001)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_subj"
      ],
      "metadata": {
        "id": "zO-NMyJxvjkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EA = EuclideanAlignment()\n",
        "X_EA = EA.convert_subject_data_with_EA(X_subj)\n",
        "X_EA"
      ],
      "metadata": {
        "id": "C_N3Iv6PiQWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groups"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxyAYsq9Wmgw",
        "outputId": "595b48c2-e85c-4309-e566-11c0fafc06aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 9, 9, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sessions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1skqALrJWoUD",
        "outputId": "a86a26bb-e344-4dd0-bde8-406e76bde690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['session_T', 'session_T', 'session_T', ..., 'session_E',\n",
              "       'session_E', 'session_E'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "runs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voL00QUTXuiM",
        "outputId": "5d006b5b-5fda-4778-caaf-a0d8fcc3e621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['run_0', 'run_0', 'run_0', ..., 'run_5', 'run_5', 'run_5'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A=runs=='run_0'"
      ],
      "metadata": {
        "id": "S-PGBqIgXv3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B=groups==1"
      ],
      "metadata": {
        "id": "H30G6LDWX9Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sRKsaUMYSyM",
        "outputId": "dad4bfde-b940-4896-c96b-e764b6a68025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2592"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(A)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZPax9ltYUzC",
        "outputId": "51d30cf3-c31b-4a74-f659-a5fd5b1fa4f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2592"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "C=np.logical_and(A,B)\n",
        "len(C)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQXxq9fdX_yG",
        "outputId": "fe7a9c07-cacc-41e8-8d12-928e992f9931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2592"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[groups==1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_jGIDtbWwKT",
        "outputId": "b4dd68ec-bdae-4b88-bbcd-61c4a5ce5f0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['run_0', 'run_0', 'run_0', 'run_0', 'run_0', 'run_0', 'run_0',\n",
              "       'run_0', 'run_0', 'run_0', 'run_0', 'run_0', 'run_0', 'run_0',\n",
              "       'run_0', 'run_0', 'run_0', 'run_0', 'run_0', 'run_0', 'run_0',\n",
              "       'run_0', 'run_0', 'run_0'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[metadata.subject.values==1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9mfeRQTG7Vt",
        "outputId": "790cc04d-49e0-4ec5-95ad-10c96acb6c0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[  6.12675343,   4.80069492,   1.8937593 , ...,  -1.80963356,\n",
              "          -0.95822005,   0.10018214],\n",
              "        [  5.58056783,   3.78074325,   1.02786715, ...,  -1.32946883,\n",
              "          -0.62293255,   0.06768451],\n",
              "        [  7.43279059,   5.46171881,   2.11057866, ...,  -0.57576039,\n",
              "          -0.65376666,  -0.73565678],\n",
              "        ...,\n",
              "        [  7.465582  ,   3.39468294,  -1.15504664, ...,  -3.41820391,\n",
              "          -2.94795975,  -2.31999583],\n",
              "        [  5.00811146,   1.58727604,  -2.11582106, ...,  -3.48512891,\n",
              "          -2.69524861,  -1.68924441],\n",
              "        [  8.01582351,   4.2274779 ,  -0.1410632 , ...,  -4.097163  ,\n",
              "          -3.54158428,  -2.65380789]],\n",
              "\n",
              "       [[ -0.7702083 ,  -0.77995427,  -1.51378905, ...,  -9.44448146,\n",
              "         -11.03344346, -11.02605606],\n",
              "        [ -1.33536169,  -1.04522174,  -0.87003607, ...,  -8.77443588,\n",
              "         -10.72309939, -10.2110244 ],\n",
              "        [  0.30316772,   0.02742408,  -1.01425588, ...,  -9.24718849,\n",
              "         -11.26148117, -10.91802947],\n",
              "        ...,\n",
              "        [ -6.81387658,  -7.2220894 ,  -7.60139055, ...,  -4.96596411,\n",
              "          -4.05442771,  -1.49866564],\n",
              "        [ -7.61409517,  -7.96167406,  -7.99530206, ...,  -1.37466745,\n",
              "          -0.81559275,   0.62729322],\n",
              "        [ -5.47673387,  -6.23518637,  -6.79171496, ...,  -2.97765015,\n",
              "          -1.1596811 ,   1.93256967]],\n",
              "\n",
              "       [[ -1.24587384,  -0.75302086,  -0.36225072, ...,  -2.82992323,\n",
              "          -3.22693962,  -3.27526469],\n",
              "        [ -3.6875027 ,  -3.11582514,  -2.04405365, ...,  -6.43561707,\n",
              "          -6.54762176,  -6.21128286],\n",
              "        [ -2.38313832,  -1.5811859 ,  -0.47598582, ...,  -4.16496599,\n",
              "          -4.23749071,  -4.22788532],\n",
              "        ...,\n",
              "        [  2.07223396,   4.62636381,   6.45390081, ...,   0.69641843,\n",
              "           0.11539997,  -0.48157866],\n",
              "        [  2.07361005,   4.36518136,   5.95973951, ...,   1.45651573,\n",
              "           1.06370282,   0.51153414],\n",
              "        [  1.98842798,   4.11490232,   5.58331719, ...,  -0.45020271,\n",
              "          -1.37723913,  -1.92878689]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[  6.59872644,   1.70126849,  -3.90740763, ...,   0.06309507,\n",
              "          -3.70758315,  -5.4388363 ],\n",
              "        [  5.45930925,   0.36041655,  -5.16098993, ...,  -4.77204777,\n",
              "          -6.33287145,  -5.5984278 ],\n",
              "        [  5.35249801,   0.31478059,  -5.06354428, ...,  -2.03284645,\n",
              "          -4.58954058,  -5.24374835],\n",
              "        ...,\n",
              "        [  7.65164957,   3.83418892,   0.11034746, ...,  -5.77195874,\n",
              "          -6.28052776,  -5.12153758],\n",
              "        [  9.43959905,   6.35714703,   3.03514519, ...,  -4.39553285,\n",
              "          -5.84319304,  -5.76223585],\n",
              "        [  7.55321517,   3.47664737,  -0.18016193, ...,  -3.44256323,\n",
              "          -3.38210688,  -2.02006842]],\n",
              "\n",
              "       [[  3.96641492,   1.21515883,  -2.29472386, ..., -10.10421233,\n",
              "          -8.24566915,  -3.79218124],\n",
              "        [  5.80626836,   4.19384841,   0.97898905, ..., -10.42033836,\n",
              "          -8.61665788,  -3.91454851],\n",
              "        [  4.83163064,   2.55749019,  -0.72910901, ..., -10.54997353,\n",
              "          -8.18615265,  -3.08138023],\n",
              "        ...,\n",
              "        [ -0.28151212,  -5.66823632, -10.79688097, ...,  -7.55421983,\n",
              "          -7.42311755,  -5.18097336],\n",
              "        [ -2.36361714,  -7.61865795, -12.16465689, ...,  -7.11591982,\n",
              "          -6.97617845,  -4.80392449],\n",
              "        [ -1.71108701,  -7.32571295, -12.3143331 , ...,  -5.98312373,\n",
              "          -5.96200294,  -4.29930532]],\n",
              "\n",
              "       [[  0.70516928,  -1.9696218 ,  -3.28150943, ...,  -3.07774664,\n",
              "          -2.15279118,  -0.72032235],\n",
              "        [ -1.12351555,  -2.83756174,  -2.7814907 , ...,  -3.06436595,\n",
              "          -2.52954674,  -1.72025114],\n",
              "        [  0.30087488,  -2.44722873,  -3.67788679, ...,  -5.00651651,\n",
              "          -3.6493291 ,  -1.50497277],\n",
              "        ...,\n",
              "        [ -2.01612753,  -2.44827552,  -1.68194009, ...,  -8.70340222,\n",
              "          -8.03403831,  -5.99541708],\n",
              "        [ -2.65958251,  -3.76009414,  -3.63224902, ...,  -7.78353027,\n",
              "          -7.15988622,  -5.3791969 ],\n",
              "        [ -3.75229019,  -2.98075739,  -0.93224665, ...,  -7.41236337,\n",
              "          -7.49863884,  -6.62258743]]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n"
      ],
      "metadata": {
        "id": "M9P4cKoufxiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from copy import deepcopy\n",
        "from time import time\n",
        "from typing import Optional, Union\n",
        "\n",
        "import numpy as np\n",
        "from mne.epochs import BaseEpochs\n",
        "from sklearn.base import clone\n",
        "from sklearn.metrics import get_scorer\n",
        "from sklearn.model_selection import (\n",
        "    LeaveOneOut,\n",
        "    LeaveOneGroupOut,\n",
        "    StratifiedKFold,\n",
        "    StratifiedShuffleSplit,\n",
        "    cross_val_score,\n",
        ")\n",
        "from sklearn.model_selection._validation import _fit_and_score, _score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "\n",
        "from moabb.evaluations.base import BaseEvaluation\n",
        "\n",
        "log = logging.getLogger(__name__)\n",
        "\n",
        "# Numpy ArrayLike is only available starting from Numpy 1.20 and Python 3.8\n",
        "Vector = Union[list, tuple, np.ndarray]"
      ],
      "metadata": {
        "id": "57tRTbF7guR-"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossCrossSubjectEvaluation(BaseEvaluation):\n",
        "    \"\"\"\n",
        "    Temporary name! \n",
        "    I want to create the evaluation that I did in the experiment 4. For this, I created one\n",
        "    classifier per subject, and evaluated using the other ones.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    paradigm : Paradigm instance\n",
        "        The paradigm to use.\n",
        "    datasets : List of Dataset instance\n",
        "        The list of dataset to run the evaluation. If none, the list of\n",
        "        compatible dataset will be retrieved from the paradigm instance.\n",
        "    random_state: int, RandomState instance, default=None\n",
        "        If not None, can guarantee same seed for shuffling examples.\n",
        "    n_jobs: int, default=1\n",
        "        Number of jobs for fitting of pipeline.\n",
        "    overwrite: bool, default=False\n",
        "        If true, overwrite the results.\n",
        "    error_score: \"raise\" or numeric, default=\"raise\"\n",
        "        Value to assign to the score if an error occurs in estimator fitting. If set to\n",
        "        'raise', the error is raised.\n",
        "    suffix: str\n",
        "        Suffix for the results file.\n",
        "    hdf5_path: str\n",
        "        Specific path for storing the results.\n",
        "    additional_columns: None\n",
        "        Adding information to results.\n",
        "    return_epochs: bool, default=False\n",
        "        use MNE epoch to train pipelines.\n",
        "    mne_labels: bool, default=False\n",
        "        if returning MNE epoch, use original dataset label if True\n",
        "    \"\"\"\n",
        "\n",
        "    def evaluate(self, dataset, pipelines):\n",
        "        if not self.is_valid(dataset):\n",
        "            raise AssertionError(\"Dataset is not appropriate for evaluation\")\n",
        "        # this is a bit akward, but we need to check if at least one pipe\n",
        "        # have to be run before loading the data. If at least one pipeline\n",
        "        # need to be run, we have to load all the data.\n",
        "        # we might need a better granularity, if we query the DB\n",
        "        run_pipes = {}\n",
        "        for subject in dataset.subject_list:\n",
        "            run_pipes.update(self.results.not_yet_computed(pipelines, dataset, subject))\n",
        "        if len(run_pipes) != 0:\n",
        "\n",
        "            # get the data\n",
        "            X, y, metadata = self.paradigm.get_data(dataset,\n",
        "                                                    return_epochs=self.return_epochs)\n",
        "\n",
        "            # encode labels\n",
        "            le = LabelEncoder()\n",
        "            y = y if self.mne_labels else le.fit_transform(y)\n",
        "\n",
        "            # extract metadata\n",
        "            groups = metadata.subject.values\n",
        "            sessions = metadata.session.values\n",
        "            n_subjects = len(dataset.subject_list)\n",
        "\n",
        "            scorer = get_scorer(self.paradigm.scoring)\n",
        "\n",
        "            # perform leave one subject out CV\n",
        "            cv = LeaveOneGroupOut()\n",
        "            # Progressbar at subject level\n",
        "            for test, train in tqdm(\n",
        "                cv.split(X, y, groups),\n",
        "                total=n_subjects,\n",
        "                desc=f\"{dataset.code}-CrossCrossSubject\"):\n",
        "\n",
        "                #aux = np.unique(groups[test])\n",
        "                #subj_0 = aux[0]\n",
        "                run_pipes = self.results.not_yet_computed(pipelines, dataset, groups[train[0]])\n",
        "\n",
        "                for name, clf in run_pipes.items():\n",
        "                    t_start = time()\n",
        "                    model = deepcopy(clf).fit(X[train], y[train])\n",
        "                    duration = time() - t_start\n",
        "\n",
        "                    # for each test subject\n",
        "                    for subject in np.unique(groups[test]):\n",
        "                        # Now evaluate\n",
        "                        ix = groups[test] == subject\n",
        "                        score = _score(model, X[test[ix]], y[test[ix]], scorer)\n",
        "                        session = 'both'\n",
        "                        nchan = (\n",
        "                            X.info[\"nchan\"] if isinstance(X, BaseEpochs) else X.shape[1]\n",
        "                        )\n",
        "                        res = {\n",
        "                            \"time\": duration,\n",
        "                            \"dataset\": dataset,\n",
        "                            \"subject\": subject,\n",
        "                            \"session\": session,\n",
        "                            \"score\": score,\n",
        "                            \"n_samples\": len(train),\n",
        "                            \"n_channels\": nchan,\n",
        "                            \"pipeline\": name,\n",
        "                        }\n",
        "\n",
        "                        yield res\n",
        "\n",
        "    def is_valid(self, dataset):\n",
        "        return len(dataset.subject_list) > 1"
      ],
      "metadata": {
        "id": "Z7UmB3i5f1OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossCrossSubjectEvaluation3(BaseEvaluation):\n",
        "    \"\"\"\n",
        "    Temporary name! \n",
        "    I want to create the evaluation that I did in the experiment 4. For this, I created one\n",
        "    classifier per subject, and evaluated using the other ones.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    paradigm : Paradigm instance\n",
        "        The paradigm to use.\n",
        "    datasets : List of Dataset instance\n",
        "        The list of dataset to run the evaluation. If none, the list of\n",
        "        compatible dataset will be retrieved from the paradigm instance.\n",
        "    random_state: int, RandomState instance, default=None\n",
        "        If not None, can guarantee same seed for shuffling examples.\n",
        "    n_jobs: int, default=1\n",
        "        Number of jobs for fitting of pipeline.\n",
        "    overwrite: bool, default=False\n",
        "        If true, overwrite the results.\n",
        "    error_score: \"raise\" or numeric, default=\"raise\"\n",
        "        Value to assign to the score if an error occurs in estimator fitting. If set to\n",
        "        'raise', the error is raised.\n",
        "    suffix: str\n",
        "        Suffix for the results file.\n",
        "    hdf5_path: str\n",
        "        Specific path for storing the results.\n",
        "    additional_columns: None\n",
        "        Adding information to results.\n",
        "    return_epochs: bool, default=False\n",
        "        use MNE epoch to train pipelines.\n",
        "    mne_labels: bool, default=False\n",
        "        if returning MNE epoch, use original dataset label if True\n",
        "    \"\"\"\n",
        "\n",
        "    def evaluate(self, dataset, pipelines):\n",
        "        if not self.is_valid(dataset):\n",
        "            raise AssertionError(\"Dataset is not appropriate for evaluation\")\n",
        "        # this is a bit akward, but we need to check if at least one pipe\n",
        "        # have to be run before loading the data. If at least one pipeline\n",
        "        # need to be run, we have to load all the data.\n",
        "        # we might need a better granularity, if we query the DB\n",
        "        run_pipes = {}\n",
        "        for subject in dataset.subject_list:\n",
        "            run_pipes.update(self.results.not_yet_computed(pipelines, dataset, subject))\n",
        "        if len(run_pipes) != 0:\n",
        "\n",
        "            # get the data\n",
        "            X, y, metadata = self.paradigm.get_data(dataset,\n",
        "                                                    return_epochs=self.return_epochs)\n",
        "\n",
        "            # encode labels\n",
        "            le = LabelEncoder()\n",
        "            y = y if self.mne_labels else le.fit_transform(y)\n",
        "\n",
        "            # extract metadata\n",
        "            groups = metadata.subject.values\n",
        "            sessions = metadata.session.values\n",
        "            n_subjects = len(dataset.subject_list)\n",
        "\n",
        "            scorer = get_scorer(self.paradigm.scoring)\n",
        "\n",
        "            # perform leave one subject out CV\n",
        "            cv = LeaveOneGroupOut()\n",
        "            # Progressbar at subject level\n",
        "            model_list=[]\n",
        "            for test, train in tqdm(\n",
        "                cv.split(X, y, groups),\n",
        "                total=n_subjects,\n",
        "                desc=f\"{dataset.code}-CrossCrossSubject\"):\n",
        "\n",
        "                #aux = np.unique(groups[test])\n",
        "                #subj_0 = aux[0]\n",
        "                run_pipes = self.results.not_yet_computed(pipelines, dataset, groups[train[0]])\n",
        "                \n",
        "                for name, clf in run_pipes.items():\n",
        "                    t_start = time()\n",
        "                    model = deepcopy(clf).fit(X[train], y[train])\n",
        "                    duration = time() - t_start\n",
        "                    model_list.append(model)\n",
        "                    \n",
        "                    # for each test subject\n",
        "                    for subject in np.unique(groups[test]):\n",
        "                        # Now evaluate\n",
        "                        ix = groups[test] == subject\n",
        "                        score = _score(model, X[test[ix]], y[test[ix]], scorer)\n",
        "                        session = 'both'\n",
        "                        nchan = (\n",
        "                            X.info[\"nchan\"] if isinstance(X, BaseEpochs) else X.shape[1]\n",
        "                        )\n",
        "                        res = {\n",
        "                            \"time\": duration,\n",
        "                            \"dataset\": dataset,\n",
        "                            \"subject\": groups[train[0]],\n",
        "                            \"test\": subject,\n",
        "                            \"session\": session,\n",
        "                            \"score\": score,\n",
        "                            \"n_samples\": len(train),\n",
        "                            \"n_channels\": nchan,\n",
        "                            \"pipeline\": name,\n",
        "                        }\n",
        "\n",
        "                        yield res\n",
        "\n",
        "    def is_valid(self, dataset):\n",
        "        return len(dataset.subject_list) > 1"
      ],
      "metadata": {
        "id": "1YLQIcxm5LG0"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train\n",
        "\n",
        "Using LOO cross-validation"
      ],
      "metadata": {
        "id": "FHTulpW0MytY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_test_column(dataset,results):\n",
        "  subj_list = dataset.subject_list\n",
        "  list_=[]\n",
        "  for i in subj_list:\n",
        "    subj_copy = copy.deepcopy(subj_list)\n",
        "    subj_copy.remove(i)\n",
        "    list_.append(subj_copy)\n",
        "\n",
        "  array=np.array(list_)\n",
        "  test=array.flatten()\n",
        "  results.insert(4,'test',test,True)\n",
        "  return results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lx68yIbvF1q0",
        "outputId": "89533801-92ba-404b-a24f-48dab4d1fc9b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 3, 4, 5, 6, 7, 8, 9, 1, 3, 4, 5, 6, 7, 8, 9, 1, 2, 4, 5, 6, 7,\n",
              "       8, 9, 1, 2, 3, 5, 6, 7, 8, 9, 1, 2, 3, 4, 6, 7, 8, 9, 1, 2, 3, 4,\n",
              "       5, 7, 8, 9, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2,\n",
              "       3, 4, 5, 6, 7, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mne.set_log_level(False)\n",
        "\n",
        "overwrite = True  # set to True if we want to overwrite cached results\n",
        "evaluation = CrossCrossSubjectEvaluation3(\n",
        "    paradigm=paradigm,\n",
        "    datasets=datasets,\n",
        "    suffix=\"braindecode_example\",\n",
        "    overwrite=overwrite,\n",
        "    return_epochs=True,\n",
        ")\n",
        "\n",
        "results = evaluation.process(pipes)\n",
        "#results = add_test_column(datasets[0],results)\n",
        "print(results.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Rb1KloL_S-s",
        "outputId": "2bb94507-a20d-463b-ec84-fa1c541dbc39"
      },
      "execution_count": 25,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r001-2014-CrossCrossSubject:   0%|          | 0/9 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  epoch    train_accuracy    train_loss      lr     dur\n",
            "-------  ----------------  ------------  ------  ------\n",
            "      1            \u001b[36m0.5000\u001b[0m        \u001b[32m0.6932\u001b[0m  0.0006  0.2660\n",
            "      2            0.5000        \u001b[32m0.6931\u001b[0m  0.0006  0.2645\n",
            "      3            0.5000        0.6947  0.0006  0.2561\n",
            "      4            0.5000        0.6946  0.0006  0.2570\n",
            "      5            0.5000        \u001b[32m0.6927\u001b[0m  0.0006  0.2605\n",
            "      6            0.5000        \u001b[32m0.6926\u001b[0m  0.0006  0.2665\n",
            "      7            0.5000        0.6928  0.0006  0.2609\n",
            "      8            0.5000        \u001b[32m0.6926\u001b[0m  0.0006  0.2550\n",
            "      9            0.5000        \u001b[32m0.6915\u001b[0m  0.0006  0.2650\n",
            "     10            0.5000        \u001b[32m0.6911\u001b[0m  0.0006  0.2640\n",
            "     11            0.5000        0.6921  0.0006  0.2708\n",
            "     12            0.5000        0.6929  0.0006  0.2621\n",
            "     13            0.5000        0.6913  0.0006  0.2640\n",
            "     14            0.5000        0.6929  0.0006  0.2641\n",
            "     15            0.5000        0.6962  0.0006  0.2636\n",
            "     16            0.5000        0.6913  0.0006  0.2599\n",
            "     17            0.5000        0.6937  0.0006  0.2680\n",
            "     18            0.5000        0.6911  0.0006  0.2611\n",
            "     19            0.5000        0.6933  0.0006  0.2686\n",
            "     20            0.5000        0.6931  0.0006  0.2747\n",
            "     21            0.5000        0.6975  0.0006  0.2802\n",
            "     22            0.5000        \u001b[32m0.6905\u001b[0m  0.0006  0.2908\n",
            "     23            0.5000        0.6964  0.0006  0.2848\n",
            "     24            0.5000        0.6931  0.0005  0.2683\n",
            "     25            0.5000        0.6938  0.0005  0.2607\n",
            "     26            0.5000        0.6949  0.0005  0.2634\n",
            "     27            0.5000        0.6941  0.0005  0.2648\n",
            "     28            0.5000        \u001b[32m0.6899\u001b[0m  0.0005  0.2666\n",
            "     29            0.5000        0.6933  0.0005  0.2696\n",
            "     30            0.5000        0.6921  0.0005  0.2654\n",
            "     31            0.5000        0.6933  0.0005  0.2651\n",
            "     32            0.5000        0.6960  0.0005  0.2619\n",
            "     33            0.5000        0.6918  0.0005  0.2828\n",
            "     34            0.5000        0.6929  0.0005  0.2594\n",
            "     35            0.5000        \u001b[32m0.6888\u001b[0m  0.0005  0.4006\n",
            "     36            0.5000        0.6944  0.0005  0.2670\n",
            "     37            0.5000        0.6895  0.0004  0.4219\n",
            "     38            0.5000        0.6909  0.0004  0.2592\n",
            "     39            0.5000        0.6951  0.0004  0.2685\n",
            "     40            0.5000        0.6924  0.0004  0.2629\n",
            "     41            0.5000        0.6937  0.0004  0.2672\n",
            "     42            0.5000        0.6907  0.0004  0.2721\n",
            "     43            0.5000        0.6916  0.0004  0.2633\n",
            "     44            0.5000        0.6915  0.0004  0.2590\n",
            "     45            0.5000        0.6929  0.0004  0.2602\n",
            "     46            0.5000        0.6965  0.0004  0.2707\n",
            "     47            0.5000        0.6904  0.0003  0.2759\n",
            "     48            0.5000        0.6896  0.0003  0.2655\n",
            "     49            0.5000        0.6934  0.0003  0.2575\n",
            "     50            0.5000        0.6896  0.0003  0.2581\n",
            "     51            0.5000        0.6893  0.0003  0.2649\n",
            "     52            0.5000        0.6900  0.0003  0.2729\n",
            "     53            0.5000        0.6902  0.0003  0.2737\n",
            "     54            0.5000        0.6928  0.0003  0.2696\n",
            "     55            0.5000        0.6950  0.0003  0.2683\n",
            "     56            0.5000        0.6894  0.0003  0.2661\n",
            "     57            0.5000        0.6905  0.0002  0.2683\n",
            "     58            0.5000        \u001b[32m0.6888\u001b[0m  0.0002  0.2713\n",
            "     59            0.5000        0.6906  0.0002  0.2608\n",
            "     60            0.5000        \u001b[32m0.6879\u001b[0m  0.0002  0.2606\n",
            "     61            0.5000        0.6894  0.0002  0.2612\n",
            "     62            0.5000        0.6906  0.0002  0.2652\n",
            "     63            0.5000        0.6949  0.0002  0.2613\n",
            "     64            0.5000        0.6885  0.0002  0.2661\n",
            "     65            0.5000        0.6923  0.0002  0.2554\n",
            "     66            0.5000        0.6930  0.0002  0.2595\n",
            "     67            0.5000        \u001b[32m0.6876\u001b[0m  0.0002  0.2665\n",
            "     68            0.5000        0.6894  0.0001  0.2660\n",
            "     69            0.5000        0.6893  0.0001  0.2665\n",
            "     70            0.5000        0.6899  0.0001  0.2683\n",
            "     71            0.5000        0.6935  0.0001  0.2839\n",
            "     72            0.5000        0.6893  0.0001  0.2804\n",
            "     73            0.5000        0.6881  0.0001  0.2648\n",
            "     74            0.5000        0.6914  0.0001  0.2689\n",
            "     75            0.5000        0.6885  0.0001  0.2564\n",
            "     76            0.5000        0.6882  0.0001  0.2618\n",
            "     77            0.5000        0.6919  0.0001  0.2695\n",
            "     78            0.5000        0.6878  0.0001  0.2649\n",
            "     79            0.5000        \u001b[32m0.6873\u001b[0m  0.0001  0.2623\n",
            "     80            0.5000        0.6881  0.0001  0.2562\n",
            "     81            0.5000        0.6890  0.0001  0.2624\n",
            "     82            0.5000        0.6915  0.0000  0.2549\n",
            "     83            0.5000        \u001b[32m0.6852\u001b[0m  0.0000  0.2549\n",
            "     84            0.5000        0.6867  0.0000  0.2651\n",
            "     85            0.5000        0.6867  0.0000  0.2713\n",
            "     86            0.5000        0.6857  0.0000  0.2639\n",
            "     87            0.5000        0.6865  0.0000  0.2620\n",
            "     88            0.5000        0.6885  0.0000  0.2688\n",
            "     89            0.5000        0.6902  0.0000  0.2592\n",
            "     90            0.5000        0.6877  0.0000  0.2589\n",
            "     91            0.5000        0.6877  0.0000  0.2672\n",
            "     92            0.5000        0.6885  0.0000  0.2612\n",
            "     93            0.5000        0.6855  0.0000  0.2810\n",
            "     94            0.5000        0.6861  0.0000  0.2622\n",
            "     95            0.5000        0.6879  0.0000  0.2713\n",
            "     96            0.5000        0.6923  0.0000  0.2813\n",
            "     97            0.5000        0.6900  0.0000  0.2688\n",
            "     98            0.5000        0.6943  0.0000  0.2641\n",
            "     99            0.5000        0.6904  0.0000  0.2668\n",
            "    100            0.5000        0.6877  0.0000  0.2584\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r001-2014-CrossCrossSubject:  11%|█         | 1/9 [01:06<08:50, 66.34s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  epoch    train_accuracy    train_loss      lr     dur\n",
            "-------  ----------------  ------------  ------  ------\n",
            "      1            \u001b[36m0.5000\u001b[0m        \u001b[32m0.6935\u001b[0m  0.0006  0.2653\n",
            "      2            0.5000        0.6943  0.0006  0.2645\n",
            "      3            0.5000        0.6937  0.0006  0.2659\n",
            "      4            0.5000        \u001b[32m0.6929\u001b[0m  0.0006  0.2627\n",
            "      5            0.5000        0.6930  0.0006  0.2590\n",
            "      6            0.5000        \u001b[32m0.6923\u001b[0m  0.0006  0.2764\n",
            "      7            0.5000        0.6928  0.0006  0.2595\n",
            "      8            0.5000        0.6935  0.0006  0.2773\n",
            "      9            0.5000        0.6942  0.0006  0.2892\n",
            "     10            0.5000        0.6937  0.0006  0.2668\n",
            "     11            0.5000        0.6935  0.0006  0.2701\n",
            "     12            0.5000        0.6945  0.0006  0.2562\n",
            "     13            0.5000        0.6939  0.0006  0.2605\n",
            "     14            0.5000        \u001b[32m0.6917\u001b[0m  0.0006  0.2661\n",
            "     15            0.5000        0.6936  0.0006  0.2637\n",
            "     16            0.5000        0.6918  0.0006  0.2694\n",
            "     17            0.5000        \u001b[32m0.6911\u001b[0m  0.0006  0.2769\n",
            "     18            0.5000        0.6918  0.0006  0.2583\n",
            "     19            0.5000        \u001b[32m0.6899\u001b[0m  0.0006  0.2566\n",
            "     20            0.5000        0.6924  0.0006  0.2669\n",
            "     21            0.5000        0.6911  0.0006  0.2681\n",
            "     22            0.5000        0.6933  0.0006  0.2660\n",
            "     23            0.5000        0.6938  0.0006  0.2677\n",
            "     24            0.5000        0.6908  0.0005  0.2642\n",
            "     25            0.5000        0.6948  0.0005  0.2634\n",
            "     26            0.5000        \u001b[32m0.6872\u001b[0m  0.0005  0.2607\n",
            "     27            0.5000        0.6928  0.0005  0.2707\n",
            "     28            0.5000        0.6961  0.0005  0.2651\n",
            "     29            0.5000        0.6954  0.0005  0.2704\n",
            "     30            0.5000        0.6925  0.0005  0.2655\n",
            "     31            0.5000        0.6882  0.0005  0.2747\n",
            "     32            0.5000        0.6944  0.0005  0.2734\n",
            "     33            0.5000        0.6928  0.0005  0.2731\n",
            "     34            0.5000        0.6920  0.0005  0.2747\n",
            "     35            0.5000        0.6897  0.0005  0.2652\n",
            "     36            0.5000        0.6916  0.0005  0.2580\n",
            "     37            0.5000        0.6930  0.0004  0.2619\n",
            "     38            0.5000        0.6900  0.0004  0.2667\n",
            "     39            0.5000        0.6929  0.0004  0.2629\n",
            "     40            0.5000        0.6902  0.0004  0.2706\n",
            "     41            0.5000        0.6919  0.0004  0.2641\n",
            "     42            0.5000        0.6939  0.0004  0.2655\n",
            "     43            0.5000        0.6916  0.0004  0.2715\n",
            "     44            0.5000        0.6916  0.0004  0.2636\n",
            "     45            0.5000        0.6907  0.0004  0.2688\n",
            "     46            0.5000        0.6894  0.0004  0.2815\n",
            "     47            0.5000        0.6921  0.0003  0.2671\n",
            "     48            0.5000        0.6895  0.0003  0.2656\n",
            "     49            0.5000        \u001b[32m0.6854\u001b[0m  0.0003  0.2673\n",
            "     50            0.5000        0.6895  0.0003  0.2635\n",
            "     51            0.5000        0.6869  0.0003  0.2601\n",
            "     52            0.5000        0.6891  0.0003  0.2641\n",
            "     53            0.5000        0.6890  0.0003  0.2646\n",
            "     54            0.5000        0.6902  0.0003  0.2603\n",
            "     55            0.5000        0.6865  0.0003  0.2800\n",
            "     56            0.5000        \u001b[32m0.6836\u001b[0m  0.0003  0.2778\n",
            "     57            0.5000        0.6849  0.0002  0.2797\n",
            "     58            0.5000        0.6842  0.0002  0.2848\n",
            "     59            0.5000        0.6898  0.0002  0.2693\n",
            "     60            0.5000        0.6881  0.0002  0.2715\n",
            "     61            0.5000        0.6902  0.0002  0.2633\n",
            "     62            0.5000        \u001b[32m0.6816\u001b[0m  0.0002  0.2617\n",
            "     63            0.5000        0.6834  0.0002  0.2570\n",
            "     64            0.5000        0.6874  0.0002  0.2619\n",
            "     65            0.5000        0.6852  0.0002  0.2570\n",
            "     66            0.5000        0.6837  0.0002  0.2591\n",
            "     67            \u001b[36m0.5903\u001b[0m        0.6840  0.0002  0.2633\n",
            "     68            0.5000        0.6859  0.0001  0.2777\n",
            "     69            0.5000        0.6822  0.0001  0.2643\n",
            "     70            0.5000        \u001b[32m0.6797\u001b[0m  0.0001  0.2681\n",
            "     71            0.5000        0.6871  0.0001  0.2659\n",
            "     72            0.5000        0.6812  0.0001  0.2717\n",
            "     73            0.5000        \u001b[32m0.6763\u001b[0m  0.0001  0.2677\n",
            "     74            0.5000        0.6824  0.0001  0.2570\n",
            "     75            0.5000        0.6841  0.0001  0.2617\n",
            "     76            0.5000        0.6843  0.0001  0.2748\n",
            "     77            0.5000        0.6780  0.0001  0.2584\n",
            "     78            0.5000        0.6852  0.0001  0.2596\n",
            "     79            0.5000        0.6791  0.0001  0.2622\n",
            "     80            0.5000        0.6831  0.0001  0.2831\n",
            "     81            0.5000        0.6772  0.0001  0.2655\n",
            "     82            0.5000        0.6782  0.0000  0.2714\n",
            "     83            0.5000        0.6854  0.0000  0.2746\n",
            "     84            0.5000        0.6787  0.0000  0.2701\n",
            "     85            0.5000        0.6807  0.0000  0.2591\n",
            "     86            0.5000        0.6801  0.0000  0.2588\n",
            "     87            0.5000        0.6768  0.0000  0.2696\n",
            "     88            0.5000        \u001b[32m0.6715\u001b[0m  0.0000  0.2655\n",
            "     89            0.5000        0.6819  0.0000  0.2675\n",
            "     90            0.5000        0.6769  0.0000  0.2675\n",
            "     91            0.5000        0.6826  0.0000  0.2632\n",
            "     92            0.5000        0.6758  0.0000  0.2607\n",
            "     93            0.5000        0.6807  0.0000  0.2597\n",
            "     94            0.5000        0.6798  0.0000  0.2700\n",
            "     95            0.5000        0.6778  0.0000  0.2637\n",
            "     96            0.5035        0.6818  0.0000  0.2547\n",
            "     97            0.5000        0.6824  0.0000  0.2577\n",
            "     98            0.5035        0.6828  0.0000  0.2618\n",
            "     99            0.5069        0.6812  0.0000  0.2646\n",
            "    100            0.5278        0.6798  0.0000  0.2701\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r001-2014-CrossCrossSubject:  22%|██▏       | 2/9 [02:12<07:42, 66.13s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  epoch    train_accuracy    train_loss      lr     dur\n",
            "-------  ----------------  ------------  ------  ------\n",
            "      1            \u001b[36m0.5000\u001b[0m        \u001b[32m0.6932\u001b[0m  0.0006  0.2711\n",
            "      2            0.5000        0.6932  0.0006  0.2681\n",
            "      3            0.5000        0.6939  0.0006  0.2571\n",
            "      4            0.5000        0.6949  0.0006  0.2665\n",
            "      5            0.5000        0.6939  0.0006  0.2587\n",
            "      6            0.5000        \u001b[32m0.6929\u001b[0m  0.0006  0.2616\n",
            "      7            0.5000        0.6933  0.0006  0.2634\n",
            "      8            0.5000        0.6932  0.0006  0.2676\n",
            "      9            0.5000        0.6945  0.0006  0.2567\n",
            "     10            0.5000        0.6929  0.0006  0.2630\n",
            "     11            0.5000        \u001b[32m0.6928\u001b[0m  0.0006  0.2625\n",
            "     12            0.5000        0.6931  0.0006  0.2634\n",
            "     13            0.5000        \u001b[32m0.6923\u001b[0m  0.0006  0.2673\n",
            "     14            0.5000        \u001b[32m0.6920\u001b[0m  0.0006  0.2738\n",
            "     15            0.5000        \u001b[32m0.6914\u001b[0m  0.0006  0.2663\n",
            "     16            0.5000        0.6945  0.0006  0.2664\n",
            "     17            0.5000        0.6935  0.0006  0.2669\n",
            "     18            0.5000        \u001b[32m0.6912\u001b[0m  0.0006  0.2713\n",
            "     19            0.5000        0.6928  0.0006  0.2648\n",
            "     20            0.5000        0.6927  0.0006  0.2772\n",
            "     21            0.5000        0.6935  0.0006  0.2739\n",
            "     22            0.5000        \u001b[32m0.6910\u001b[0m  0.0006  0.2642\n",
            "     23            0.5000        0.6910  0.0006  0.2660\n",
            "     24            0.5000        0.6943  0.0005  0.2625\n",
            "     25            \u001b[36m0.6111\u001b[0m        0.6914  0.0005  0.2689\n",
            "     26            0.5000        0.6934  0.0005  0.2582\n",
            "     27            0.5000        0.6919  0.0005  0.2699\n",
            "     28            0.5000        0.6914  0.0005  0.2625\n",
            "     29            0.5000        0.6949  0.0005  0.2671\n",
            "     30            0.5000        0.6917  0.0005  0.2743\n",
            "     31            0.5000        \u001b[32m0.6904\u001b[0m  0.0005  0.2796\n",
            "     32            0.5000        0.6938  0.0005  0.2665\n",
            "     33            0.5000        0.6938  0.0005  0.2643\n",
            "     34            0.5000        0.6911  0.0005  0.2599\n",
            "     35            0.5000        0.6912  0.0005  0.2640\n",
            "     36            0.5000        0.6924  0.0005  0.2639\n",
            "     37            0.5000        0.6924  0.0004  0.2708\n",
            "     38            0.5000        \u001b[32m0.6884\u001b[0m  0.0004  0.2622\n",
            "     39            0.5000        0.6905  0.0004  0.2582\n",
            "     40            0.5000        0.6918  0.0004  0.2664\n",
            "     41            0.5000        0.6910  0.0004  0.2619\n",
            "     42            0.5000        0.6906  0.0004  0.2628\n",
            "     43            0.5000        0.6904  0.0004  0.2662\n",
            "     44            0.5000        0.6900  0.0004  0.2710\n",
            "     45            0.5000        0.6933  0.0004  0.2742\n",
            "     46            0.5000        0.6920  0.0004  0.2808\n",
            "     47            0.5000        \u001b[32m0.6878\u001b[0m  0.0003  0.2675\n",
            "     48            0.5000        \u001b[32m0.6874\u001b[0m  0.0003  0.2668\n",
            "     49            0.5000        \u001b[32m0.6843\u001b[0m  0.0003  0.2620\n",
            "     50            0.5000        0.6848  0.0003  0.2654\n",
            "     51            0.5000        0.6878  0.0003  0.2606\n",
            "     52            0.5000        0.6852  0.0003  0.2658\n",
            "     53            0.5000        0.6894  0.0003  0.2573\n",
            "     54            0.5000        0.6906  0.0003  0.2479\n",
            "     55            0.5000        \u001b[32m0.6835\u001b[0m  0.0003  0.2519\n",
            "     56            0.5000        \u001b[32m0.6807\u001b[0m  0.0003  0.2499\n",
            "     57            0.5000        0.6840  0.0002  0.2574\n",
            "     58            0.5000        0.6817  0.0002  0.2793\n",
            "     59            0.5000        0.6843  0.0002  0.2712\n",
            "     60            0.5000        \u001b[32m0.6771\u001b[0m  0.0002  0.2606\n",
            "     61            0.5000        0.6824  0.0002  0.2647\n",
            "     62            0.5000        0.6808  0.0002  0.2545\n",
            "     63            0.5000        0.6779  0.0002  0.2513\n",
            "     64            0.5000        0.6823  0.0002  0.2520\n",
            "     65            0.5000        0.6792  0.0002  0.2559\n",
            "     66            0.5000        0.6788  0.0002  0.2549\n",
            "     67            0.5000        0.6777  0.0002  0.2752\n",
            "     68            0.5000        0.6812  0.0001  0.2776\n",
            "     69            0.5000        0.6772  0.0001  0.2735\n",
            "     70            0.5000        0.6777  0.0001  0.2836\n",
            "     71            0.5000        \u001b[32m0.6768\u001b[0m  0.0001  0.2885\n",
            "     72            0.5000        \u001b[32m0.6737\u001b[0m  0.0001  0.2661\n",
            "     73            0.5000        0.6742  0.0001  0.2664\n",
            "     74            0.5000        0.6743  0.0001  0.2783\n",
            "     75            0.5000        0.6819  0.0001  0.2608\n",
            "     76            0.5000        0.6747  0.0001  0.2621\n",
            "     77            0.5000        0.6751  0.0001  0.2784\n",
            "     78            0.5000        0.6761  0.0001  0.2655\n",
            "     79            0.5000        0.6774  0.0001  0.2675\n",
            "     80            0.5000        \u001b[32m0.6733\u001b[0m  0.0001  0.2744\n",
            "     81            0.5000        \u001b[32m0.6690\u001b[0m  0.0001  0.2663\n",
            "     82            0.5000        0.6753  0.0000  0.2688\n",
            "     83            0.5000        0.6737  0.0000  0.2655\n",
            "     84            0.5000        0.6713  0.0000  0.2666\n",
            "     85            0.5000        0.6713  0.0000  0.2680\n",
            "     86            0.5000        0.6731  0.0000  0.2674\n",
            "     87            0.5000        0.6698  0.0000  0.2624\n",
            "     88            0.5000        0.6814  0.0000  0.2673\n",
            "     89            0.5000        0.6706  0.0000  0.2720\n",
            "     90            0.5000        0.6702  0.0000  0.2699\n",
            "     91            0.5000        0.6699  0.0000  0.2685\n",
            "     92            0.5000        0.6733  0.0000  0.2688\n",
            "     93            0.5000        0.6739  0.0000  0.3003\n",
            "     94            0.5000        0.6746  0.0000  0.2824\n",
            "     95            0.5000        0.6702  0.0000  0.2835\n",
            "     96            0.5000        \u001b[32m0.6646\u001b[0m  0.0000  0.2819\n",
            "     97            0.5000        0.6710  0.0000  0.2632\n",
            "     98            0.5000        0.6768  0.0000  0.2623\n",
            "     99            0.5000        0.6651  0.0000  0.2631\n",
            "    100            0.5000        0.6680  0.0000  0.2561\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r001-2014-CrossCrossSubject:  33%|███▎      | 3/9 [03:17<06:35, 65.89s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  epoch    train_accuracy    train_loss      lr     dur\n",
            "-------  ----------------  ------------  ------  ------\n",
            "      1            \u001b[36m0.5000\u001b[0m        \u001b[32m0.6936\u001b[0m  0.0006  0.2659\n",
            "      2            0.5000        \u001b[32m0.6932\u001b[0m  0.0006  0.2647\n",
            "      3            0.5000        0.6934  0.0006  0.2617\n",
            "      4            0.5000        0.6940  0.0006  0.2735\n",
            "      5            0.5000        \u001b[32m0.6924\u001b[0m  0.0006  0.2604\n",
            "      6            0.5000        \u001b[32m0.6922\u001b[0m  0.0006  0.2712\n",
            "      7            0.5000        0.6927  0.0006  0.2640\n",
            "      8            0.5000        \u001b[32m0.6916\u001b[0m  0.0006  0.2540\n",
            "      9            0.5000        0.6936  0.0006  0.2540\n",
            "     10            0.5000        0.6939  0.0006  0.2486\n",
            "     11            0.5000        0.6931  0.0006  0.2470\n",
            "     12            0.5000        0.6928  0.0006  0.2435\n",
            "     13            0.5000        0.6920  0.0006  0.2367\n",
            "     14            0.5000        0.6939  0.0006  0.2428\n",
            "     15            0.5000        0.6961  0.0006  0.2441\n",
            "     16            0.5000        0.6922  0.0006  0.2390\n",
            "     17            0.5000        0.6919  0.0006  0.2516\n",
            "     18            0.5000        0.6949  0.0006  0.2493\n",
            "     19            0.5000        0.6975  0.0006  0.2508\n",
            "     20            0.5000        0.6921  0.0006  0.2647\n",
            "     21            0.5000        0.6947  0.0006  0.2554\n",
            "     22            0.5000        \u001b[32m0.6911\u001b[0m  0.0006  0.2687\n",
            "     23            0.5000        0.6915  0.0006  0.2664\n",
            "     24            0.5000        0.6915  0.0005  0.2733\n",
            "     25            0.5000        0.6940  0.0005  0.2584\n",
            "     26            0.5000        0.6915  0.0005  0.2679\n",
            "     27            0.5000        0.6941  0.0005  0.2595\n",
            "     28            0.5000        0.6927  0.0005  0.2713\n",
            "     29            0.5000        0.6912  0.0005  0.2833\n",
            "     30            0.5000        \u001b[32m0.6895\u001b[0m  0.0005  0.2697\n",
            "     31            0.5000        0.6937  0.0005  0.2691\n",
            "     32            0.5000        0.6934  0.0005  0.2647\n",
            "     33            0.5000        0.6927  0.0005  0.2610\n",
            "     34            0.5000        0.6952  0.0005  0.2541\n",
            "     35            0.5000        0.6903  0.0005  0.2477\n",
            "     36            0.5000        0.6943  0.0005  0.2539\n",
            "     37            0.5000        0.6915  0.0004  0.2495\n",
            "     38            0.5000        0.6914  0.0004  0.2495\n",
            "     39            0.5000        0.6918  0.0004  0.2590\n",
            "     40            0.5000        0.6910  0.0004  0.2624\n",
            "     41            0.5000        0.6905  0.0004  0.2529\n",
            "     42            0.5000        \u001b[32m0.6894\u001b[0m  0.0004  0.2512\n",
            "     43            0.5000        0.6932  0.0004  0.2443\n",
            "     44            0.5000        0.6914  0.0004  0.2494\n",
            "     45            0.5000        \u001b[32m0.6887\u001b[0m  0.0004  0.2454\n",
            "     46            0.5000        \u001b[32m0.6886\u001b[0m  0.0004  0.2502\n",
            "     47            0.5000        \u001b[32m0.6858\u001b[0m  0.0003  0.2562\n",
            "     48            0.5000        0.6915  0.0003  0.2612\n",
            "     49            0.5000        0.6894  0.0003  0.2617\n",
            "     50            0.5000        0.6896  0.0003  0.2629\n",
            "     51            0.5000        0.6925  0.0003  0.2544\n",
            "     52            0.5000        \u001b[32m0.6852\u001b[0m  0.0003  0.2658\n",
            "     53            0.5000        0.6889  0.0003  0.2644\n",
            "     54            0.5000        0.6868  0.0003  0.2852\n",
            "     55            0.5000        0.6866  0.0003  0.2844\n",
            "     56            0.5000        0.6855  0.0003  0.2775\n",
            "     57            0.5000        0.6856  0.0002  0.2616\n",
            "     58            0.5000        \u001b[32m0.6850\u001b[0m  0.0002  0.2647\n",
            "     59            0.5000        \u001b[32m0.6821\u001b[0m  0.0002  0.2596\n",
            "     60            0.5000        0.6864  0.0002  0.2503\n",
            "     61            0.5000        0.6848  0.0002  0.2475\n",
            "     62            0.5000        0.6856  0.0002  0.2521\n",
            "     63            0.5000        0.6858  0.0002  0.2464\n",
            "     64            0.5000        0.6825  0.0002  0.2469\n",
            "     65            0.5000        0.6908  0.0002  0.2416\n",
            "     66            0.5000        0.6835  0.0002  0.2484\n",
            "     67            0.5000        0.6846  0.0002  0.2487\n",
            "     68            0.5000        0.6848  0.0001  0.2513\n",
            "     69            0.5000        \u001b[32m0.6808\u001b[0m  0.0001  0.2592\n",
            "     70            0.5000        \u001b[32m0.6760\u001b[0m  0.0001  0.2728\n",
            "     71            0.5000        0.6805  0.0001  0.2591\n",
            "     72            0.5000        0.6804  0.0001  0.2596\n",
            "     73            0.5000        0.6813  0.0001  0.2580\n",
            "     74            0.5000        0.6794  0.0001  0.2502\n",
            "     75            0.5000        0.6805  0.0001  0.2341\n",
            "     76            0.5000        0.6781  0.0001  0.2411\n",
            "     77            0.5000        \u001b[32m0.6705\u001b[0m  0.0001  0.2437\n",
            "     78            0.5000        0.6781  0.0001  0.2435\n",
            "     79            0.5000        0.6826  0.0001  0.2707\n",
            "     80            0.5000        0.6791  0.0001  0.2800\n",
            "     81            0.5000        0.6822  0.0001  0.2818\n",
            "     82            0.5000        0.6783  0.0000  0.2650\n",
            "     83            0.5000        0.6801  0.0000  0.2658\n",
            "     84            0.5000        0.6775  0.0000  0.2686\n",
            "     85            0.5000        0.6797  0.0000  0.2781\n",
            "     86            0.5000        0.6729  0.0000  0.2640\n",
            "     87            0.5000        0.6742  0.0000  0.2574\n",
            "     88            0.5000        0.6743  0.0000  0.2517\n",
            "     89            0.5000        0.6775  0.0000  0.2574\n",
            "     90            0.5000        0.6767  0.0000  0.2590\n",
            "     91            0.5000        0.6775  0.0000  0.2637\n",
            "     92            0.5000        0.6763  0.0000  0.2582\n",
            "     93            0.5000        0.6780  0.0000  0.2553\n",
            "     94            0.5000        0.6807  0.0000  0.2590\n",
            "     95            0.5000        0.6802  0.0000  0.2482\n",
            "     96            0.5000        0.6817  0.0000  0.2414\n",
            "     97            0.5000        0.6836  0.0000  0.2400\n",
            "     98            0.5000        0.6773  0.0000  0.2451\n",
            "     99            0.5000        0.6763  0.0000  0.2405\n",
            "    100            0.5000        0.6735  0.0000  0.2598\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r001-2014-CrossCrossSubject:  44%|████▍     | 4/9 [04:22<05:26, 65.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_accuracy    train_loss      lr     dur\n",
            "-------  ----------------  ------------  ------  ------\n",
            "      1            \u001b[36m0.5000\u001b[0m        \u001b[32m0.6933\u001b[0m  0.0006  0.2619\n",
            "      2            0.5000        \u001b[32m0.6921\u001b[0m  0.0006  0.2598\n",
            "      3            0.5000        0.6945  0.0006  0.2577\n",
            "      4            0.5000        0.6943  0.0006  0.2505\n",
            "      5            0.5000        0.6934  0.0006  0.2573\n",
            "      6            0.5000        0.6932  0.0006  0.2572\n",
            "      7            0.5000        0.6929  0.0006  0.2594\n",
            "      8            0.5000        0.6934  0.0006  0.2692\n",
            "      9            0.5000        0.6929  0.0006  0.2532\n",
            "     10            0.5000        0.6928  0.0006  0.2511\n",
            "     11            0.5000        0.6950  0.0006  0.2496\n",
            "     12            0.5000        0.6930  0.0006  0.2533\n",
            "     13            0.5000        0.6928  0.0006  0.2576\n",
            "     14            0.5000        0.6937  0.0006  0.2744\n",
            "     15            0.5000        \u001b[32m0.6915\u001b[0m  0.0006  0.2812\n",
            "     16            0.5000        0.6955  0.0006  0.2905\n",
            "     17            0.5000        0.6935  0.0006  0.2729\n",
            "     18            0.5000        0.6939  0.0006  0.2641\n",
            "     19            0.5000        0.6938  0.0006  0.2599\n",
            "     20            0.5000        \u001b[32m0.6915\u001b[0m  0.0006  0.2581\n",
            "     21            0.5000        0.6952  0.0006  0.2483\n",
            "     22            0.5000        0.6943  0.0006  0.2594\n",
            "     23            0.5000        0.6928  0.0006  0.2519\n",
            "     24            0.5000        0.6927  0.0005  0.2532\n",
            "     25            0.5000        0.6918  0.0005  0.2659\n",
            "     26            0.5000        0.6952  0.0005  0.2644\n",
            "     27            0.5000        0.6937  0.0005  0.2587\n",
            "     28            0.5000        0.6928  0.0005  0.2584\n",
            "     29            0.5000        0.6927  0.0005  0.2608\n",
            "     30            0.5000        0.6937  0.0005  0.2664\n",
            "     31            0.5000        0.6925  0.0005  0.2660\n",
            "     32            0.5000        0.6955  0.0005  0.2611\n",
            "     33            0.5000        0.6932  0.0005  0.2647\n",
            "     34            0.5000        \u001b[32m0.6904\u001b[0m  0.0005  0.2653\n",
            "     35            0.5000        0.6926  0.0005  0.2575\n",
            "     36            0.5000        0.6951  0.0005  0.2774\n",
            "     37            0.5000        0.6935  0.0004  0.2639\n",
            "     38            0.5000        0.6922  0.0004  0.2643\n",
            "     39            0.5000        0.6925  0.0004  0.2606\n",
            "     40            0.5000        0.6944  0.0004  0.2757\n",
            "     41            0.5000        0.6910  0.0004  0.2752\n",
            "     42            0.5000        0.6941  0.0004  0.2646\n",
            "     43            0.5000        0.6921  0.0004  0.2680\n",
            "     44            0.5000        0.6907  0.0004  0.2606\n",
            "     45            0.5000        \u001b[32m0.6891\u001b[0m  0.0004  0.2565\n",
            "     46            0.5000        0.6904  0.0004  0.2556\n",
            "     47            0.5000        0.6942  0.0003  0.2616\n",
            "     48            0.5000        0.6932  0.0003  0.2603\n",
            "     49            0.5000        0.6918  0.0003  0.2508\n",
            "     50            0.5000        0.6925  0.0003  0.2509\n",
            "     51            0.5000        0.6919  0.0003  0.2468\n",
            "     52            0.5000        0.6922  0.0003  0.2499\n",
            "     53            0.5000        0.6908  0.0003  0.2423\n",
            "     54            0.5000        0.6910  0.0003  0.2454\n",
            "     55            0.5000        0.6925  0.0003  0.2442\n",
            "     56            0.5000        0.6930  0.0003  0.2357\n",
            "     57            0.5000        0.6895  0.0002  0.2396\n",
            "     58            0.5000        0.6914  0.0002  0.2528\n",
            "     59            0.5000        0.6920  0.0002  0.2470\n",
            "     60            0.5000        \u001b[32m0.6876\u001b[0m  0.0002  0.2565\n",
            "     61            0.5000        0.6911  0.0002  0.2533\n",
            "     62            0.5000        0.6906  0.0002  0.2527\n",
            "     63            0.5000        0.6900  0.0002  0.2661\n",
            "     64            0.5000        0.6906  0.0002  0.2679\n",
            "     65            0.5000        0.6895  0.0002  0.2731\n",
            "     66            0.5000        0.6912  0.0002  0.2727\n",
            "     67            0.5000        \u001b[32m0.6872\u001b[0m  0.0002  0.2645\n",
            "     68            0.5000        0.6906  0.0001  0.2741\n",
            "     69            0.5000        0.6917  0.0001  0.2723\n",
            "     70            0.5000        0.6932  0.0001  0.2735\n",
            "     71            0.5000        0.6919  0.0001  0.2693\n",
            "     72            0.5000        0.6911  0.0001  0.2643\n",
            "     73            0.5000        0.6897  0.0001  0.2655\n",
            "     74            0.5000        0.6895  0.0001  0.2679\n",
            "     75            0.5000        0.6901  0.0001  0.2718\n",
            "     76            0.5000        0.6914  0.0001  0.2788\n",
            "     77            0.5000        0.6898  0.0001  0.2653\n",
            "     78            0.5000        0.6910  0.0001  0.2612\n",
            "     79            0.5000        0.6917  0.0001  0.2669\n",
            "     80            0.5000        0.6901  0.0001  0.2703\n",
            "     81            0.5000        0.6913  0.0001  0.2683\n",
            "     82            0.5000        0.6896  0.0000  0.2624\n",
            "     83            0.5000        0.6918  0.0000  0.2630\n",
            "     84            0.5000        0.6883  0.0000  0.2615\n",
            "     85            0.5000        0.6884  0.0000  0.2634\n",
            "     86            0.5000        0.6907  0.0000  0.2668\n",
            "     87            0.5000        0.6914  0.0000  0.2690\n",
            "     88            0.5000        0.6903  0.0000  0.2684\n",
            "     89            0.5000        0.6910  0.0000  0.2754\n",
            "     90            0.5000        0.6906  0.0000  0.2731\n",
            "     91            0.5000        0.6903  0.0000  0.2567\n",
            "     92            0.5000        0.6929  0.0000  0.2601\n",
            "     93            0.5000        0.6908  0.0000  0.2609\n",
            "     94            0.5000        0.6925  0.0000  0.2623\n",
            "     95            0.5000        0.6891  0.0000  0.2636\n",
            "     96            0.5000        0.6897  0.0000  0.2559\n",
            "     97            0.5000        0.6923  0.0000  0.2593\n",
            "     98            0.5000        0.6884  0.0000  0.2632\n",
            "     99            0.5000        0.6895  0.0000  0.2552\n",
            "    100            0.5000        0.6939  0.0000  0.2649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r001-2014-CrossCrossSubject:  56%|█████▌    | 5/9 [05:26<04:20, 65.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_accuracy    train_loss      lr     dur\n",
            "-------  ----------------  ------------  ------  ------\n",
            "      1            \u001b[36m0.5000\u001b[0m        \u001b[32m0.6931\u001b[0m  0.0006  0.2661\n",
            "      2            0.5000        \u001b[32m0.6926\u001b[0m  0.0006  0.2622\n",
            "      3            0.5000        \u001b[32m0.6924\u001b[0m  0.0006  0.2659\n",
            "      4            0.5000        \u001b[32m0.6922\u001b[0m  0.0006  0.2699\n",
            "      5            0.5000        0.6935  0.0006  0.2625\n",
            "      6            0.5000        \u001b[32m0.6911\u001b[0m  0.0006  0.2664\n",
            "      7            0.5000        0.6949  0.0006  0.2597\n",
            "      8            0.5000        0.6919  0.0006  0.2620\n",
            "      9            0.5000        0.6921  0.0006  0.2636\n",
            "     10            0.5000        0.6944  0.0006  0.2660\n",
            "     11            0.5000        0.6930  0.0006  0.2651\n",
            "     12            0.5000        0.6926  0.0006  0.2661\n",
            "     13            0.5000        0.6935  0.0006  0.2605\n",
            "     14            0.5000        0.6918  0.0006  0.2618\n",
            "     15            0.5000        0.6950  0.0006  0.2633\n",
            "     16            0.5000        0.6952  0.0006  0.2685\n",
            "     17            0.5000        0.6938  0.0006  0.2648\n",
            "     18            0.5000        0.6912  0.0006  0.2621\n",
            "     19            0.5000        0.6926  0.0006  0.2577\n",
            "     20            0.5000        0.6925  0.0006  0.2579\n",
            "     21            0.5000        0.6921  0.0006  0.2544\n",
            "     22            0.5000        \u001b[32m0.6906\u001b[0m  0.0006  0.2616\n",
            "     23            0.5000        0.6922  0.0006  0.2760\n",
            "     24            0.5000        0.6910  0.0005  0.2616\n",
            "     25            0.5000        0.6946  0.0005  0.2911\n",
            "     26            0.5000        \u001b[32m0.6871\u001b[0m  0.0005  0.2764\n",
            "     27            0.5000        0.6896  0.0005  0.2589\n",
            "     28            0.5000        0.6919  0.0005  0.2642\n",
            "     29            0.5000        0.6901  0.0005  0.2639\n",
            "     30            0.5000        0.6928  0.0005  0.2646\n",
            "     31            0.5000        0.6925  0.0005  0.2638\n",
            "     32            0.5000        \u001b[32m0.6870\u001b[0m  0.0005  0.2610\n",
            "     33            0.5000        0.6870  0.0005  0.2717\n",
            "     34            0.5000        0.6900  0.0005  0.2669\n",
            "     35            0.5000        0.6894  0.0005  0.2719\n",
            "     36            0.5000        0.6891  0.0005  0.2605\n",
            "     37            0.5000        0.6892  0.0004  0.2616\n",
            "     38            0.5000        \u001b[32m0.6867\u001b[0m  0.0004  0.2639\n",
            "     39            0.5000        0.6869  0.0004  0.2565\n",
            "     40            0.5000        0.6876  0.0004  0.2632\n",
            "     41            0.5000        \u001b[32m0.6812\u001b[0m  0.0004  0.2557\n",
            "     42            0.5000        0.6856  0.0004  0.2594\n",
            "     43            0.5000        0.6838  0.0004  0.2649\n",
            "     44            0.5000        0.6830  0.0004  0.2558\n",
            "     45            0.5000        \u001b[32m0.6794\u001b[0m  0.0004  0.2594\n",
            "     46            0.5000        \u001b[32m0.6787\u001b[0m  0.0004  0.2617\n",
            "     47            0.5000        0.6859  0.0003  0.2576\n",
            "     48            0.5000        0.6796  0.0003  0.2566\n",
            "     49            0.5000        0.6794  0.0003  0.2687\n",
            "     50            0.5000        \u001b[32m0.6734\u001b[0m  0.0003  0.2605\n",
            "     51            0.5000        \u001b[32m0.6729\u001b[0m  0.0003  0.2487\n",
            "     52            0.5000        0.6739  0.0003  0.2555\n",
            "     53            0.5000        \u001b[32m0.6716\u001b[0m  0.0003  0.2599\n",
            "     54            0.5000        0.6728  0.0003  0.2706\n",
            "     55            0.5000        \u001b[32m0.6667\u001b[0m  0.0003  0.2568\n",
            "     56            0.5000        0.6671  0.0003  0.2480\n",
            "     57            0.5000        \u001b[32m0.6641\u001b[0m  0.0002  0.2514\n",
            "     58            0.5000        \u001b[32m0.6587\u001b[0m  0.0002  0.2616\n",
            "     59            0.5000        0.6641  0.0002  0.2581\n",
            "     60            0.5000        0.6612  0.0002  0.2674\n",
            "     61            0.5000        \u001b[32m0.6528\u001b[0m  0.0002  0.2684\n",
            "     62            0.5000        0.6547  0.0002  0.2695\n",
            "     63            0.5000        0.6551  0.0002  0.2710\n",
            "     64            0.5000        \u001b[32m0.6513\u001b[0m  0.0002  0.2625\n",
            "     65            0.5000        0.6585  0.0002  0.2700\n",
            "     66            0.5000        \u001b[32m0.6504\u001b[0m  0.0002  0.2687\n",
            "     67            0.5000        0.6516  0.0002  0.2540\n",
            "     68            0.5000        \u001b[32m0.6454\u001b[0m  0.0001  0.2736\n",
            "     69            0.5000        0.6503  0.0001  0.2598\n",
            "     70            0.5000        \u001b[32m0.6445\u001b[0m  0.0001  0.2575\n",
            "     71            0.5000        0.6520  0.0001  0.2536\n",
            "     72            0.5000        0.6604  0.0001  0.2868\n",
            "     73            0.5000        0.6491  0.0001  0.2702\n",
            "     74            0.5000        \u001b[32m0.6385\u001b[0m  0.0001  0.2739\n",
            "     75            0.5000        0.6495  0.0001  0.2593\n",
            "     76            0.5000        0.6474  0.0001  0.2567\n",
            "     77            0.5000        0.6512  0.0001  0.2613\n",
            "     78            0.5000        0.6461  0.0001  0.2545\n",
            "     79            0.5000        \u001b[32m0.6348\u001b[0m  0.0001  0.2667\n",
            "     80            0.5000        0.6422  0.0001  0.2700\n",
            "     81            0.5000        0.6502  0.0001  0.2665\n",
            "     82            0.5000        0.6489  0.0000  0.2672\n",
            "     83            0.5000        0.6528  0.0000  0.2659\n",
            "     84            0.5000        0.6401  0.0000  0.2592\n",
            "     85            0.5000        0.6397  0.0000  0.2760\n",
            "     86            0.5000        0.6354  0.0000  0.2602\n",
            "     87            0.5000        0.6427  0.0000  0.2643\n",
            "     88            0.5000        0.6389  0.0000  0.2618\n",
            "     89            0.5000        \u001b[32m0.6331\u001b[0m  0.0000  0.2564\n",
            "     90            0.5000        0.6468  0.0000  0.2659\n",
            "     91            0.5000        0.6353  0.0000  0.2576\n",
            "     92            0.5000        \u001b[32m0.6323\u001b[0m  0.0000  0.2592\n",
            "     93            0.5000        0.6427  0.0000  0.2595\n",
            "     94            0.5000        0.6454  0.0000  0.2603\n",
            "     95            0.5000        0.6432  0.0000  0.2621\n",
            "     96            0.5000        0.6550  0.0000  0.2832\n",
            "     97            0.5000        0.6464  0.0000  0.2652\n",
            "     98            0.5000        0.6421  0.0000  0.2687\n",
            "     99            0.5000        0.6558  0.0000  0.2601\n",
            "    100            0.5000        0.6462  0.0000  0.2581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r001-2014-CrossCrossSubject:  67%|██████▋   | 6/9 [06:31<03:15, 65.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_accuracy    train_loss      lr     dur\n",
            "-------  ----------------  ------------  ------  ------\n",
            "      1            \u001b[36m0.5000\u001b[0m        \u001b[32m0.6934\u001b[0m  0.0006  0.2651\n",
            "      2            0.5000        \u001b[32m0.6931\u001b[0m  0.0006  0.2794\n",
            "      3            0.5000        0.6934  0.0006  0.2686\n",
            "      4            0.5000        0.6932  0.0006  0.2682\n",
            "      5            0.5000        0.6936  0.0006  0.2569\n",
            "      6            0.5000        \u001b[32m0.6930\u001b[0m  0.0006  0.2573\n",
            "      7            0.5000        \u001b[32m0.6920\u001b[0m  0.0006  0.2688\n",
            "      8            0.5000        0.6940  0.0006  0.2722\n",
            "      9            0.5000        0.6938  0.0006  0.2719\n",
            "     10            0.5000        \u001b[32m0.6904\u001b[0m  0.0006  0.2544\n",
            "     11            0.5000        0.6952  0.0006  0.2480\n",
            "     12            0.5000        \u001b[32m0.6900\u001b[0m  0.0006  0.2490\n",
            "     13            0.5000        0.6934  0.0006  0.2475\n",
            "     14            0.5000        0.6932  0.0006  0.2461\n",
            "     15            0.5000        0.6960  0.0006  0.2456\n",
            "     16            0.5000        0.6946  0.0006  0.2404\n",
            "     17            0.5000        \u001b[32m0.6892\u001b[0m  0.0006  0.2488\n",
            "     18            0.5000        0.6927  0.0006  0.2420\n",
            "     19            0.5000        0.6906  0.0006  0.2560\n",
            "     20            0.5000        0.6919  0.0006  0.2615\n",
            "     21            0.5000        0.6972  0.0006  0.2624\n",
            "     22            0.5000        0.6934  0.0006  0.2596\n",
            "     23            0.5000        0.6923  0.0006  0.2565\n",
            "     24            0.5000        0.6946  0.0005  0.2613\n",
            "     25            0.5000        0.6917  0.0005  0.2565\n",
            "     26            0.5000        0.6932  0.0005  0.2675\n",
            "     27            0.5000        0.6939  0.0005  0.2608\n",
            "     28            0.5000        0.6916  0.0005  0.2571\n",
            "     29            0.5000        0.6902  0.0005  0.2556\n",
            "     30            0.5000        0.6940  0.0005  0.2566\n",
            "     31            0.5000        0.6910  0.0005  0.2804\n",
            "     32            0.5000        0.6955  0.0005  0.2701\n",
            "     33            0.5000        0.6927  0.0005  0.2923\n",
            "     34            0.5000        0.6913  0.0005  0.2625\n",
            "     35            0.5000        0.6913  0.0005  0.2609\n",
            "     36            0.5000        \u001b[32m0.6884\u001b[0m  0.0005  0.2599\n",
            "     37            0.5000        0.6909  0.0004  0.2682\n",
            "     38            0.5000        0.6894  0.0004  0.2621\n",
            "     39            0.5000        0.6911  0.0004  0.2681\n",
            "     40            0.5000        0.6905  0.0004  0.2631\n",
            "     41            0.5000        \u001b[32m0.6881\u001b[0m  0.0004  0.2545\n",
            "     42            0.5000        0.6896  0.0004  0.2641\n",
            "     43            0.5000        \u001b[32m0.6878\u001b[0m  0.0004  0.2612\n",
            "     44            0.5000        \u001b[32m0.6875\u001b[0m  0.0004  0.2581\n",
            "     45            0.5000        \u001b[32m0.6873\u001b[0m  0.0004  0.2574\n",
            "     46            0.5000        \u001b[32m0.6872\u001b[0m  0.0004  0.2549\n",
            "     47            0.5000        \u001b[32m0.6865\u001b[0m  0.0003  0.2626\n",
            "     48            0.5000        0.6869  0.0003  0.2702\n",
            "     49            0.5000        \u001b[32m0.6815\u001b[0m  0.0003  0.2671\n",
            "     50            0.5000        0.6869  0.0003  0.2618\n",
            "     51            0.5000        0.6825  0.0003  0.2636\n",
            "     52            0.5000        0.6823  0.0003  0.2682\n",
            "     53            0.5000        \u001b[32m0.6791\u001b[0m  0.0003  0.3089\n",
            "     54            0.5000        0.6820  0.0003  0.2737\n",
            "     55            0.5000        0.6822  0.0003  0.2904\n",
            "     56            0.5000        0.6819  0.0003  0.2741\n",
            "     57            0.5000        \u001b[32m0.6763\u001b[0m  0.0002  0.2798\n",
            "     58            0.5000        0.6767  0.0002  0.2624\n",
            "     59            0.5000        0.6789  0.0002  0.2631\n",
            "     60            0.5000        0.6770  0.0002  0.2660\n",
            "     61            0.5000        \u001b[32m0.6751\u001b[0m  0.0002  0.2571\n",
            "     62            0.5000        \u001b[32m0.6748\u001b[0m  0.0002  0.2564\n",
            "     63            0.5000        0.6748  0.0002  0.2500\n",
            "     64            0.5000        0.6756  0.0002  0.2689\n",
            "     65            0.5000        \u001b[32m0.6713\u001b[0m  0.0002  0.2471\n",
            "     66            0.5000        0.6758  0.0002  0.2524\n",
            "     67            0.5000        \u001b[32m0.6702\u001b[0m  0.0002  0.2566\n",
            "     68            0.5000        0.6744  0.0001  0.2629\n",
            "     69            0.5000        \u001b[32m0.6683\u001b[0m  0.0001  0.2657\n",
            "     70            0.5000        0.6683  0.0001  0.2563\n",
            "     71            0.5000        0.6704  0.0001  0.2517\n",
            "     72            0.5000        \u001b[32m0.6618\u001b[0m  0.0001  0.2512\n",
            "     73            0.5000        0.6666  0.0001  0.2566\n",
            "     74            0.5000        \u001b[32m0.6613\u001b[0m  0.0001  0.2639\n",
            "     75            0.5000        0.6718  0.0001  0.2610\n",
            "     76            0.5000        \u001b[32m0.6603\u001b[0m  0.0001  0.2577\n",
            "     77            0.5000        0.6700  0.0001  0.2614\n",
            "     78            0.5000        0.6605  0.0001  0.2580\n",
            "     79            0.5000        0.6609  0.0001  0.2604\n",
            "     80            0.5000        0.6678  0.0001  0.2784\n",
            "     81            0.5000        0.6677  0.0001  0.2759\n",
            "     82            0.5000        0.6707  0.0000  0.2806\n",
            "     83            0.5000        0.6715  0.0000  0.2581\n",
            "     84            0.5000        0.6648  0.0000  0.2623\n",
            "     85            0.5000        0.6641  0.0000  0.2615\n",
            "     86            0.5000        0.6627  0.0000  0.2588\n",
            "     87            0.5000        0.6683  0.0000  0.2534\n",
            "     88            0.5000        0.6616  0.0000  0.2568\n",
            "     89            0.5000        0.6637  0.0000  0.2628\n",
            "     90            0.5000        0.6620  0.0000  0.2552\n",
            "     91            0.5000        \u001b[32m0.6584\u001b[0m  0.0000  0.2586\n",
            "     92            0.5000        0.6593  0.0000  0.2585\n",
            "     93            0.5000        0.6696  0.0000  0.2493\n",
            "     94            0.5000        \u001b[32m0.6582\u001b[0m  0.0000  0.2451\n",
            "     95            0.5000        \u001b[32m0.6582\u001b[0m  0.0000  0.2483\n",
            "     96            0.5000        0.6634  0.0000  0.2532\n",
            "     97            0.5000        0.6699  0.0000  0.2436\n",
            "     98            0.5000        0.6690  0.0000  0.2493\n",
            "     99            0.5000        0.6595  0.0000  0.2479\n",
            "    100            0.5000        0.6599  0.0000  0.2463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r001-2014-CrossCrossSubject:  78%|███████▊  | 7/9 [07:37<02:10, 65.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_accuracy    train_loss      lr     dur\n",
            "-------  ----------------  ------------  ------  ------\n",
            "      1            \u001b[36m0.5000\u001b[0m        \u001b[32m0.6927\u001b[0m  0.0006  0.2679\n",
            "      2            0.5000        \u001b[32m0.6926\u001b[0m  0.0006  0.2587\n",
            "      3            0.5000        0.6942  0.0006  0.2586\n",
            "      4            0.5000        0.6928  0.0006  0.2598\n",
            "      5            0.5000        0.6929  0.0006  0.2492\n",
            "      6            0.5000        \u001b[32m0.6917\u001b[0m  0.0006  0.2471\n",
            "      7            0.5000        0.6948  0.0006  0.2507\n",
            "      8            0.5000        0.6935  0.0006  0.2513\n",
            "      9            0.5000        0.6921  0.0006  0.2526\n",
            "     10            0.5000        0.6931  0.0006  0.2586\n",
            "     11            0.5000        0.6928  0.0006  0.2651\n",
            "     12            0.5000        0.6947  0.0006  0.2515\n",
            "     13            0.5000        0.6924  0.0006  0.2591\n",
            "     14            0.5000        0.6923  0.0006  0.2659\n",
            "     15            0.5000        \u001b[32m0.6905\u001b[0m  0.0006  0.2563\n",
            "     16            0.5000        0.6922  0.0006  0.2661\n",
            "     17            0.5000        0.6960  0.0006  0.2754\n",
            "     18            0.5000        0.6908  0.0006  0.2755\n",
            "     19            0.5000        0.6912  0.0006  0.2605\n",
            "     20            0.5000        0.6941  0.0006  0.2478\n",
            "     21            0.5000        0.6910  0.0006  0.2556\n",
            "     22            0.5000        \u001b[32m0.6900\u001b[0m  0.0006  0.2536\n",
            "     23            0.5000        0.6936  0.0006  0.2557\n",
            "     24            0.5000        0.6911  0.0005  0.2663\n",
            "     25            0.5000        0.6933  0.0005  0.2686\n",
            "     26            0.5000        \u001b[32m0.6893\u001b[0m  0.0005  0.2591\n",
            "     27            0.5000        0.6962  0.0005  0.2575\n",
            "     28            0.5000        0.6929  0.0005  0.2614\n",
            "     29            0.5000        0.6915  0.0005  0.2613\n",
            "     30            0.5000        0.6908  0.0005  0.2670\n",
            "     31            0.5000        0.6924  0.0005  0.2713\n",
            "     32            0.5000        \u001b[32m0.6888\u001b[0m  0.0005  0.2650\n",
            "     33            0.5000        0.6894  0.0005  0.2632\n",
            "     34            0.5000        0.6908  0.0005  0.2523\n",
            "     35            0.5000        0.6903  0.0005  0.2555\n",
            "     36            0.5000        0.6900  0.0005  0.2498\n",
            "     37            0.5000        0.6914  0.0004  0.2539\n",
            "     38            0.5000        \u001b[32m0.6867\u001b[0m  0.0004  0.2601\n",
            "     39            0.5000        0.6883  0.0004  0.2808\n",
            "     40            0.5000        \u001b[32m0.6831\u001b[0m  0.0004  0.2748\n",
            "     41            0.5000        0.6873  0.0004  0.2864\n",
            "     42            0.5000        \u001b[32m0.6821\u001b[0m  0.0004  0.2882\n",
            "     43            0.5000        0.6864  0.0004  0.2705\n",
            "     44            0.5000        \u001b[32m0.6817\u001b[0m  0.0004  0.2671\n",
            "     45            0.5000        0.6835  0.0004  0.2644\n",
            "     46            0.5000        0.6905  0.0004  0.2549\n",
            "     47            0.5000        0.6820  0.0003  0.2591\n",
            "     48            0.5000        \u001b[32m0.6796\u001b[0m  0.0003  0.2544\n",
            "     49            0.5000        \u001b[32m0.6787\u001b[0m  0.0003  0.2534\n",
            "     50            0.5000        0.6808  0.0003  0.2548\n",
            "     51            0.5000        \u001b[32m0.6777\u001b[0m  0.0003  0.2593\n",
            "     52            0.5000        \u001b[32m0.6705\u001b[0m  0.0003  0.2578\n",
            "     53            0.5000        0.6742  0.0003  0.2667\n",
            "     54            0.5000        0.6741  0.0003  0.2610\n",
            "     55            0.5000        0.6771  0.0003  0.2678\n",
            "     56            0.5000        0.6732  0.0003  0.2812\n",
            "     57            0.5000        0.6756  0.0002  0.2653\n",
            "     58            0.5000        \u001b[32m0.6681\u001b[0m  0.0002  0.2673\n",
            "     59            0.5000        0.6713  0.0002  0.2629\n",
            "     60            0.5000        \u001b[32m0.6582\u001b[0m  0.0002  0.2619\n",
            "     61            0.5000        0.6746  0.0002  0.2618\n",
            "     62            0.5000        0.6612  0.0002  0.2543\n",
            "     63            0.5000        0.6646  0.0002  0.2572\n",
            "     64            0.5000        0.6627  0.0002  0.2758\n",
            "     65            0.5000        0.6670  0.0002  0.2764\n",
            "     66            0.5000        0.6738  0.0002  0.2854\n",
            "     67            0.5000        \u001b[32m0.6543\u001b[0m  0.0002  0.2771\n",
            "     68            0.5000        0.6659  0.0001  0.2646\n",
            "     69            0.5000        0.6643  0.0001  0.2693\n",
            "     70            0.5000        \u001b[32m0.6541\u001b[0m  0.0001  0.2646\n",
            "     71            0.5000        0.6590  0.0001  0.2698\n",
            "     72            0.5000        0.6571  0.0001  0.2608\n",
            "     73            0.5000        0.6586  0.0001  0.2619\n",
            "     74            0.5000        0.6616  0.0001  0.2575\n",
            "     75            0.5000        0.6557  0.0001  0.2585\n",
            "     76            0.5000        0.6554  0.0001  0.2603\n",
            "     77            0.5000        0.6624  0.0001  0.2550\n",
            "     78            0.5000        0.6605  0.0001  0.2632\n",
            "     79            0.5000        \u001b[32m0.6529\u001b[0m  0.0001  0.2741\n",
            "     80            0.5000        0.6588  0.0001  0.2684\n",
            "     81            0.5000        \u001b[32m0.6445\u001b[0m  0.0001  0.2578\n",
            "     82            0.5000        0.6545  0.0000  0.2607\n",
            "     83            0.5000        0.6564  0.0000  0.2564\n",
            "     84            0.5000        0.6587  0.0000  0.2538\n",
            "     85            0.5000        0.6626  0.0000  0.2519\n",
            "     86            0.5000        0.6459  0.0000  0.2591\n",
            "     87            0.5000        0.6525  0.0000  0.2597\n",
            "     88            0.5000        0.6603  0.0000  0.2724\n",
            "     89            0.5000        \u001b[32m0.6444\u001b[0m  0.0000  0.2795\n",
            "     90            0.5000        0.6591  0.0000  0.2807\n",
            "     91            0.5000        0.6577  0.0000  0.2696\n",
            "     92            0.5000        0.6560  0.0000  0.2620\n",
            "     93            0.5000        0.6525  0.0000  0.2611\n",
            "     94            0.5000        0.6612  0.0000  0.2567\n",
            "     95            0.5000        0.6558  0.0000  0.2554\n",
            "     96            0.5000        0.6487  0.0000  0.2572\n",
            "     97            0.5000        0.6578  0.0000  0.2497\n",
            "     98            0.5000        0.6500  0.0000  0.2443\n",
            "     99            0.5000        0.6585  0.0000  0.2411\n",
            "    100            0.5000        0.6562  0.0000  0.2478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r001-2014-CrossCrossSubject:  89%|████████▉ | 8/9 [08:41<01:05, 65.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_accuracy    train_loss      lr     dur\n",
            "-------  ----------------  ------------  ------  ------\n",
            "      1            \u001b[36m0.5000\u001b[0m        \u001b[32m0.6922\u001b[0m  0.0006  0.2672\n",
            "      2            0.5000        0.6938  0.0006  0.2639\n",
            "      3            0.5000        \u001b[32m0.6921\u001b[0m  0.0006  0.2613\n",
            "      4            0.5000        0.6926  0.0006  0.2509\n",
            "      5            0.5000        \u001b[32m0.6914\u001b[0m  0.0006  0.2569\n",
            "      6            0.5000        \u001b[32m0.6913\u001b[0m  0.0006  0.2505\n",
            "      7            0.5000        0.6940  0.0006  0.2560\n",
            "      8            0.5000        0.6925  0.0006  0.2578\n",
            "      9            0.5000        0.6945  0.0006  0.2637\n",
            "     10            0.5000        0.6916  0.0006  0.2634\n",
            "     11            0.5000        0.6925  0.0006  0.2569\n",
            "     12            0.5000        \u001b[32m0.6910\u001b[0m  0.0006  0.2587\n",
            "     13            0.5000        0.6959  0.0006  0.2627\n",
            "     14            0.5000        0.6929  0.0006  0.2672\n",
            "     15            0.5000        0.6952  0.0006  0.2606\n",
            "     16            0.5000        \u001b[32m0.6902\u001b[0m  0.0006  0.2568\n",
            "     17            0.5000        \u001b[32m0.6901\u001b[0m  0.0006  0.2623\n",
            "     18            0.5000        0.6944  0.0006  0.2574\n",
            "     19            0.5000        \u001b[32m0.6898\u001b[0m  0.0006  0.2617\n",
            "     20            0.5000        \u001b[32m0.6882\u001b[0m  0.0006  0.2705\n",
            "     21            0.5000        0.6916  0.0006  0.2615\n",
            "     22            0.5000        0.6891  0.0006  0.2612\n",
            "     23            0.5000        0.6891  0.0006  0.2507\n",
            "     24            0.5000        \u001b[32m0.6844\u001b[0m  0.0005  0.2639\n",
            "     25            0.5000        \u001b[32m0.6828\u001b[0m  0.0005  0.2788\n",
            "     26            0.5000        0.6840  0.0005  0.2855\n",
            "     27            0.5000        \u001b[32m0.6818\u001b[0m  0.0005  0.2630\n",
            "     28            0.5000        0.6839  0.0005  0.2682\n",
            "     29            0.5000        \u001b[32m0.6758\u001b[0m  0.0005  0.2681\n",
            "     30            0.5000        0.6827  0.0005  0.2715\n",
            "     31            0.5000        \u001b[32m0.6750\u001b[0m  0.0005  0.2701\n",
            "     32            0.5000        \u001b[32m0.6693\u001b[0m  0.0005  0.2617\n",
            "     33            0.5000        0.6751  0.0005  0.2616\n",
            "     34            0.5000        \u001b[32m0.6607\u001b[0m  0.0005  0.2727\n",
            "     35            0.5000        0.6657  0.0005  0.2669\n",
            "     36            0.5000        0.6645  0.0005  0.2707\n",
            "     37            0.5000        0.6668  0.0004  0.2601\n",
            "     38            0.5000        0.6656  0.0004  0.2558\n",
            "     39            0.5000        \u001b[32m0.6593\u001b[0m  0.0004  0.2624\n",
            "     40            0.5000        \u001b[32m0.6430\u001b[0m  0.0004  0.2573\n",
            "     41            0.5000        0.6434  0.0004  0.2565\n",
            "     42            0.5000        0.6576  0.0004  0.2512\n",
            "     43            0.5000        0.6636  0.0004  0.2564\n",
            "     44            0.5000        \u001b[32m0.6307\u001b[0m  0.0004  0.2513\n",
            "     45            0.5000        0.6535  0.0004  0.2665\n",
            "     46            0.5000        0.6391  0.0004  0.2712\n",
            "     47            0.5000        0.6326  0.0003  0.2746\n",
            "     48            0.5000        0.6444  0.0003  0.2715\n",
            "     49            0.5000        0.6544  0.0003  0.2760\n",
            "     50            0.5000        0.6455  0.0003  0.2729\n",
            "     51            0.5000        0.6506  0.0003  0.2565\n",
            "     52            0.5000        0.6451  0.0003  0.2540\n",
            "     53            0.5000        0.6492  0.0003  0.2561\n",
            "     54            0.5000        \u001b[32m0.6244\u001b[0m  0.0003  0.2575\n",
            "     55            0.5000        0.6632  0.0003  0.2495\n",
            "     56            0.5000        0.6453  0.0003  0.2528\n",
            "     57            0.5000        0.6360  0.0002  0.2548\n",
            "     58            0.5000        0.6285  0.0002  0.2589\n",
            "     59            0.5000        \u001b[32m0.6234\u001b[0m  0.0002  0.2625\n",
            "     60            0.5000        0.6324  0.0002  0.2723\n",
            "     61            0.5000        \u001b[32m0.6058\u001b[0m  0.0002  0.2621\n",
            "     62            0.5000        0.6288  0.0002  0.2633\n",
            "     63            0.5000        0.6256  0.0002  0.2603\n",
            "     64            0.5000        0.6262  0.0002  0.2587\n",
            "     65            0.5000        0.6339  0.0002  0.2657\n",
            "     66            0.5000        0.6223  0.0002  0.2682\n",
            "     67            0.5000        0.6274  0.0002  0.2592\n",
            "     68            0.5000        \u001b[32m0.6051\u001b[0m  0.0001  0.2575\n",
            "     69            0.5000        0.6334  0.0001  0.2548\n",
            "     70            0.5000        0.6405  0.0001  0.2571\n",
            "     71            0.5000        0.6280  0.0001  0.2652\n",
            "     72            0.5000        0.6271  0.0001  0.2906\n",
            "     73            0.5000        0.6322  0.0001  0.2765\n",
            "     74            0.5000        0.6341  0.0001  0.2820\n",
            "     75            0.5000        0.6299  0.0001  0.2631\n",
            "     76            0.5000        0.6397  0.0001  0.2657\n",
            "     77            0.5000        0.6307  0.0001  0.2556\n",
            "     78            0.5000        0.6104  0.0001  0.2398\n",
            "     79            0.5000        0.6193  0.0001  0.2449\n",
            "     80            0.5000        0.6123  0.0001  0.2497\n",
            "     81            0.5000        0.6169  0.0001  0.2554\n",
            "     82            0.5000        0.6143  0.0000  0.2582\n",
            "     83            0.5000        0.6369  0.0000  0.2618\n",
            "     84            0.5000        0.6170  0.0000  0.2577\n",
            "     85            0.5000        0.6391  0.0000  0.2505\n",
            "     86            0.5000        0.6171  0.0000  0.2481\n",
            "     87            0.5000        0.6172  0.0000  0.2503\n",
            "     88            0.5000        0.6351  0.0000  0.2577\n",
            "     89            0.5000        0.6303  0.0000  0.2562\n",
            "     90            0.5000        0.6378  0.0000  0.2744\n",
            "     91            0.5000        0.6284  0.0000  0.2656\n",
            "     92            0.5000        0.6112  0.0000  0.2567\n",
            "     93            0.5000        0.6337  0.0000  0.2558\n",
            "     94            0.5000        0.6193  0.0000  0.2558\n",
            "     95            0.5000        0.6427  0.0000  0.2544\n",
            "     96            0.5000        0.6363  0.0000  0.2533\n",
            "     97            0.5000        0.6533  0.0000  0.2652\n",
            "     98            0.5000        0.6067  0.0000  0.2798\n",
            "     99            0.5000        0.6233  0.0000  0.2764\n",
            "    100            0.5000        0.6288  0.0000  0.2622\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "001-2014-CrossCrossSubject: 100%|██████████| 9/9 [09:46<00:00, 65.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      score       time  samples subject session  channels  n_sessions  \\\n",
            "0  0.452643  50.132069    288.0       1    both        22           2   \n",
            "1  0.466556  50.132069    288.0       1    both        22           2   \n",
            "2  0.496648  50.132069    288.0       1    both        22           2   \n",
            "3  0.509163  50.132069    288.0       1    both        22           2   \n",
            "4  0.538604  50.132069    288.0       1    both        22           2   \n",
            "\n",
            "    dataset  pipeline  \n",
            "0  001-2014  EEGNetv4  \n",
            "1  001-2014  EEGNetv4  \n",
            "2  001-2014  EEGNetv4  \n",
            "3  001-2014  EEGNetv4  \n",
            "4  001-2014  EEGNetv4  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1656
        },
        "id": "07-vonD3h7BD",
        "outputId": "7e9c35cc-00ba-4ccc-bcc3-9715ef78ea8e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       score       time  samples subject  test session  channels  n_sessions  \\\n",
              "0   0.452643  50.132069    288.0       1     2    both        22           2   \n",
              "1   0.466556  50.132069    288.0       1     3    both        22           2   \n",
              "2   0.496648  50.132069    288.0       1     4    both        22           2   \n",
              "3   0.509163  50.132069    288.0       1     5    both        22           2   \n",
              "4   0.538604  50.132069    288.0       1     6    both        22           2   \n",
              "..       ...        ...      ...     ...   ...     ...       ...         ...   \n",
              "67  0.493152  48.950687    288.0       9     4    both        22           2   \n",
              "68  0.438368  48.950687    288.0       9     5    both        22           2   \n",
              "69  0.588445  48.950687    288.0       9     6    both        22           2   \n",
              "70  0.531202  48.950687    288.0       9     7    both        22           2   \n",
              "71  0.522955  48.950687    288.0       9     8    both        22           2   \n",
              "\n",
              "     dataset  pipeline  \n",
              "0   001-2014  EEGNetv4  \n",
              "1   001-2014  EEGNetv4  \n",
              "2   001-2014  EEGNetv4  \n",
              "3   001-2014  EEGNetv4  \n",
              "4   001-2014  EEGNetv4  \n",
              "..       ...       ...  \n",
              "67  001-2014  EEGNetv4  \n",
              "68  001-2014  EEGNetv4  \n",
              "69  001-2014  EEGNetv4  \n",
              "70  001-2014  EEGNetv4  \n",
              "71  001-2014  EEGNetv4  \n",
              "\n",
              "[72 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dac8313d-7c4a-420e-8813-9a6f072a7e59\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>score</th>\n",
              "      <th>time</th>\n",
              "      <th>samples</th>\n",
              "      <th>subject</th>\n",
              "      <th>test</th>\n",
              "      <th>session</th>\n",
              "      <th>channels</th>\n",
              "      <th>n_sessions</th>\n",
              "      <th>dataset</th>\n",
              "      <th>pipeline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.452643</td>\n",
              "      <td>50.132069</td>\n",
              "      <td>288.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>both</td>\n",
              "      <td>22</td>\n",
              "      <td>2</td>\n",
              "      <td>001-2014</td>\n",
              "      <td>EEGNetv4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.466556</td>\n",
              "      <td>50.132069</td>\n",
              "      <td>288.0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>both</td>\n",
              "      <td>22</td>\n",
              "      <td>2</td>\n",
              "      <td>001-2014</td>\n",
              "      <td>EEGNetv4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.496648</td>\n",
              "      <td>50.132069</td>\n",
              "      <td>288.0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>both</td>\n",
              "      <td>22</td>\n",
              "      <td>2</td>\n",
              "      <td>001-2014</td>\n",
              "      <td>EEGNetv4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.509163</td>\n",
              "      <td>50.132069</td>\n",
              "      <td>288.0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>both</td>\n",
              "      <td>22</td>\n",
              "      <td>2</td>\n",
              "      <td>001-2014</td>\n",
              "      <td>EEGNetv4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.538604</td>\n",
              "      <td>50.132069</td>\n",
              "      <td>288.0</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>both</td>\n",
              "      <td>22</td>\n",
              "      <td>2</td>\n",
              "      <td>001-2014</td>\n",
              "      <td>EEGNetv4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>0.493152</td>\n",
              "      <td>48.950687</td>\n",
              "      <td>288.0</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>both</td>\n",
              "      <td>22</td>\n",
              "      <td>2</td>\n",
              "      <td>001-2014</td>\n",
              "      <td>EEGNetv4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>0.438368</td>\n",
              "      <td>48.950687</td>\n",
              "      <td>288.0</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>both</td>\n",
              "      <td>22</td>\n",
              "      <td>2</td>\n",
              "      <td>001-2014</td>\n",
              "      <td>EEGNetv4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>0.588445</td>\n",
              "      <td>48.950687</td>\n",
              "      <td>288.0</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>both</td>\n",
              "      <td>22</td>\n",
              "      <td>2</td>\n",
              "      <td>001-2014</td>\n",
              "      <td>EEGNetv4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>0.531202</td>\n",
              "      <td>48.950687</td>\n",
              "      <td>288.0</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>both</td>\n",
              "      <td>22</td>\n",
              "      <td>2</td>\n",
              "      <td>001-2014</td>\n",
              "      <td>EEGNetv4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>0.522955</td>\n",
              "      <td>48.950687</td>\n",
              "      <td>288.0</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>both</td>\n",
              "      <td>22</td>\n",
              "      <td>2</td>\n",
              "      <td>001-2014</td>\n",
              "      <td>EEGNetv4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>72 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dac8313d-7c4a-420e-8813-9a6f072a7e59')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dac8313d-7c4a-420e-8813-9a6f072a7e59 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dac8313d-7c4a-420e-8813-9a6f072a7e59');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize"
      ],
      "metadata": {
        "id": "XP4HNSVcXFU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##############################################################################\n",
        "# Plot Results\n",
        "# ----------------\n",
        "#\n",
        "# Here we plot the results. We the first plot is a pointplot with the average\n",
        "# performance of each pipeline across session and subjects.\n",
        "# The second plot is a paired scatter plot. Each point representing the score\n",
        "# of a single session. An algorithm will outperforms another is most of the\n",
        "# points are in its quadrant.\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=[8, 4], sharey=True)\n",
        "\n",
        "sns.stripplot(\n",
        "    data=results,\n",
        "    y=\"score\",\n",
        "    x=\"subject\",\n",
        "    ax=axes[0],\n",
        "    jitter=True,\n",
        "    alpha=0.5,\n",
        "    zorder=1,\n",
        "    palette=\"Set1\",\n",
        ")\n",
        "sns.pointplot(\n",
        "    data=results, y=\"score\", x=\"subject\", ax=axes[0], zorder=1, palette=\"Set1\"\n",
        ")\n",
        "\n",
        "axes[0].set_ylabel(\"ROC AUC\")\n",
        "axes[0].set_ylim(0.3, 0.8)\n",
        "\n",
        "# paired plot\n",
        "paired = results.pivot_table(\n",
        "    values=\"score\", columns=\"pipeline\", index=[\"subject\", \"session\"]\n",
        ")\n",
        "paired = paired.reset_index()\n",
        "\n",
        "axes[1].plot([0, 1], [0, 1], ls=\"--\", c=\"k\")\n",
        "axes[1].set_xlim(0.3, 0.8)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I-Z03NEIAjrB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "6fbb032e-304f-4842-ec44-087f25ae048c"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}